{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyO1esJJ7l1SDkhrUtutW4UN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tnwls6865/cau_deep_learning/blob/main/LM_and_RNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F1JRXRDlzF_t",
        "outputId": "bf191e83-6aac-4731-b41b-74123b15f5a4"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ilZM3c3fySfp"
      },
      "outputs": [],
      "source": [
        "from nltk import bigrams, ngrams\n",
        "\n",
        "text = [['점심', '메뉴', '추천'], ['점심', '추천', '저녁', '추천', '파스타', '갈비탕']]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "list(bigrams(text[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WvnYQv5gylnQ",
        "outputId": "7870b5e2-00a8-4810-a1e2-06f3933582ed"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('점심', '메뉴'), ('메뉴', '추천')]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "list(bigrams(text[1]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ttWP_BdUyl5c",
        "outputId": "21e87dff-dffa-4f77-fb01-6ba88641c2cd"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('점심', '추천'), ('추천', '저녁'), ('저녁', '추천'), ('추천', '파스타'), ('파스타', '갈비탕')]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "list(ngrams(text[1], n=2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OmUD7g8tymid",
        "outputId": "4b306ccc-0a0b-4771-d7cd-804b57b202e2"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('점심', '추천'), ('추천', '저녁'), ('저녁', '추천'), ('추천', '파스타'), ('파스타', '갈비탕')]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "list(ngrams(text[1], n=3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "To5uvDQ9ynFd",
        "outputId": "83502993-ddf9-4569-d433-a48312e9867a"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('점심', '추천', '저녁'),\n",
              " ('추천', '저녁', '추천'),\n",
              " ('저녁', '추천', '파스타'),\n",
              " ('추천', '파스타', '갈비탕')]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.util import pad_sequence\n",
        "list(pad_sequence(text[0],\n",
        "                  pad_left=True, left_pad_symbol=\"<s>\",\n",
        "                  pad_right=True, right_pad_symbol=\"</s>\",\n",
        "                  n=2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IpSb1r3qynty",
        "outputId": "3bbcc98a-d921-47ad-ac89-f92dfc29e384"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['<s>', '점심', '메뉴', '추천', '</s>']"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "padded_sent = list(pad_sequence(text[0], pad_left=True, left_pad_symbol=\"<s>\",\n",
        "                                pad_right=True, right_pad_symbol=\"</s>\", n=2))\n",
        "list(ngrams(padded_sent, n=2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pi4kg9cyyvAB",
        "outputId": "095de356-bd2f-4b24-87df-57463537398f"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('<s>', '점심'), ('점심', '메뉴'), ('메뉴', '추천'), ('추천', '</s>')]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.lm.preprocessing import pad_both_ends\n",
        "list(pad_both_ends(text[0], n=2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ENddgn1oyySz",
        "outputId": "27a6d82d-7eb1-4e4b-8352-8b956b0c28d1"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['<s>', '점심', '메뉴', '추천', '</s>']"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.util import everygrams\n",
        "padded_bigrams = list(pad_both_ends(text[0], n=2))\n",
        "list(everygrams(padded_bigrams, max_len=3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yxt5ErPqyzC5",
        "outputId": "8805a699-4f3b-4609-9e96-c9364f84b284"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('<s>',),\n",
              " ('<s>', '점심'),\n",
              " ('<s>', '점심', '메뉴'),\n",
              " ('점심',),\n",
              " ('점심', '메뉴'),\n",
              " ('점심', '메뉴', '추천'),\n",
              " ('메뉴',),\n",
              " ('메뉴', '추천'),\n",
              " ('메뉴', '추천', '</s>'),\n",
              " ('추천',),\n",
              " ('추천', '</s>'),\n",
              " ('</s>',)]"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.lm.preprocessing import flatten\n",
        "list(flatten(pad_both_ends(sent, n=2) for sent in text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LL7O88p9zBNu",
        "outputId": "50e9050c-fb1f-4343-e5b7-e0a47442712f"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['<s>',\n",
              " '점심',\n",
              " '메뉴',\n",
              " '추천',\n",
              " '</s>',\n",
              " '<s>',\n",
              " '점심',\n",
              " '추천',\n",
              " '저녁',\n",
              " '추천',\n",
              " '파스타',\n",
              " '갈비탕',\n",
              " '</s>']"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
        "\n",
        "training_ngrams, padded_sentences = padded_everygram_pipeline(2, text)\n",
        "for ngramlize_sent in training_ngrams:\n",
        "    print(list(ngramlize_sent))\n",
        "    print()\n",
        "print('#############')\n",
        "list(padded_sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GjjRWr2zzCJH",
        "outputId": "a8da1d34-b0ec-47ed-ba39-ece6d848562b"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('<s>',), ('<s>', '점심'), ('점심',), ('점심', '메뉴'), ('메뉴',), ('메뉴', '추천'), ('추천',), ('추천', '</s>'), ('</s>',)]\n",
            "\n",
            "[('<s>',), ('<s>', '점심'), ('점심',), ('점심', '추천'), ('추천',), ('추천', '저녁'), ('저녁',), ('저녁', '추천'), ('추천',), ('추천', '파스타'), ('파스타',), ('파스타', '갈비탕'), ('갈비탕',), ('갈비탕', '</s>'), ('</s>',)]\n",
            "\n",
            "#############\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['<s>',\n",
              " '점심',\n",
              " '메뉴',\n",
              " '추천',\n",
              " '</s>',\n",
              " '<s>',\n",
              " '점심',\n",
              " '추천',\n",
              " '저녁',\n",
              " '추천',\n",
              " '파스타',\n",
              " '갈비탕',\n",
              " '</s>']"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import word_tokenize, sent_tokenize\n",
        "word_tokenize(sent_tokenize(\"This is a foobar sentence. Yes it is.\")[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jmyvmj-pzC-_",
        "outputId": "793899fb-3ec8-4b68-9e63-5454114e9351"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['This', 'is', 'a', 'foobar', 'sentence', '.']"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(sent_tokenize(\"This is a foobar sentence. Yes it is.\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nx6A_sgrzD4S",
        "outputId": "2f824a45-14d7-446d-f763-e6a79e26d77c"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['This is a foobar sentence.', 'Yes it is.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(word_tokenize('This is a foobar sentence.'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4rB15SI_zJlf",
        "outputId": "293a71c4-6992-45b0-dbea-372f37f17d96"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['This', 'is', 'a', 'foobar', 'sentence', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import io\n",
        "import requests\n",
        "\n",
        "url = \"https://gist.githubusercontent.com/alvations/53b01e4076573fea47c6057120bb017a/raw/b01ff96a5f76848450e648f35da6497ca9454e4a/language-never-random.txt\"\n",
        "text = requests.get(url).content.decode('utf8')\n",
        "with io.open('language-never-random.txt', 'w', encoding='utf8') as fout:\n",
        "    fout.write(text)"
      ],
      "metadata": {
        "id": "Crrl-gfjzJ37"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize the text.\n",
        "tokenized_text = [list(map(str.lower, word_tokenize(sent)))\n",
        "                  for sent in sent_tokenize(text)]\n",
        "print(tokenized_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jqqYzMtzzKjU",
        "outputId": "9716ec71-11cd-4e35-ed53-c0903c7a39db"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['language', 'is', 'never', ',', 'ever', ',', 'ever', ',', 'random', 'adam', 'kilgarriff', 'abstract', 'language', 'users', 'never', 'choose', 'words', 'randomly', ',', 'and', 'language', 'is', 'essentially', 'non-random', '.'], ['statistical', 'hypothesis', 'testing', 'uses', 'a', 'null', 'hypothesis', ',', 'which', 'posits', 'randomness', '.'], ['hence', ',', 'when', 'we', 'look', 'at', 'linguistic', 'phenomena', 'in', 'cor-', 'pora', ',', 'the', 'null', 'hypothesis', 'will', 'never', 'be', 'true', '.'], ['moreover', ',', 'where', 'there', 'is', 'enough', 'data', ',', 'we', 'shall', '(', 'almost', ')', 'always', 'be', 'able', 'to', 'establish', 'that', 'it', 'is', 'not', 'true', '.'], ['in', 'corpus', 'studies', ',', 'we', 'frequently', 'do', 'have', 'enough', 'data', ',', 'so', 'the', 'fact', 'that', 'a', 'rela-', 'tion', 'between', 'two', 'phenomena', 'is', 'demonstrably', 'non-random', ',', 'does', 'not', 'sup-', 'port', 'the', 'inference', 'that', 'it', 'is', 'not', 'arbitrary', '.'], ['we', 'present', 'experimental', 'evidence', 'of', 'how', 'arbitrary', 'associations', 'between', 'word', 'frequencies', 'and', 'corpora', 'are', 'systematically', 'non-random', '.'], ['we', 'review', 'literature', 'in', 'which', 'hypothesis', 'test-', 'ing', 'has', 'been', 'used', ',', 'and', 'show', 'how', 'it', 'has', 'often', 'led', 'to', 'unhelpful', 'or', 'mislead-', 'ing', 'results', '.'], ['keywords', ':', '쎲쎲쎲', '1', '.'], ['introduction', 'any', 'two', 'phenomena', 'might', 'or', 'might', 'not', 'be', 'related', '.'], ['the', 'range', 'of', 'pos-', 'sibilities', 'is', 'that', 'the', 'association', 'is', 'random', ',', 'arbitrary', ',', 'motivated', 'or', 'pre-', 'dictable', '(', 'r', ',', 'a', ',', 'm', ',', 'p', ')', '.'], ['the', 'bulk', 'of', 'linguistic', 'questions', 'concern', 'the', 'dis-', 'tinction', 'between', 'a', 'and', 'm.', 'a', 'linguistic', 'account', 'of', 'a', 'phenomenon', 'gen-', 'erally', 'gives', 'us', 'reason', 'to', 'view', 'the', 'relation', 'between', ',', 'for', 'example', ',', 'a', 'verb', '’', 's', 'syntax', 'and', 'its', 'semantics', ',', 'as', 'motivated', 'rather', 'than', 'arbitrary', '.'], ['however', ',', 'it', 'is', 'not', 'in', 'general', 'possible', 'to', 'model', 'the', 'a-m', 'distinction', 'mathematically', '.'], ['the', 'distinction', 'that', 'can', 'be', 'modeled', 'mathematically', 'is', 'between', 'r', 'and', 'not-r', ',', 'that', 'is', ',', 'between', 'random', ',', 'or', 'uncorrelated', ',', 'pairs', 'and', 'pairs', 'where', 'there', 'is', 'some', 'correlation', ',', 'be', 'it', 'arbitrary', ',', 'motivated', 'or', 'predictable.1', 'the', 'mechanism', 'here', 'is', 'hypothesis-testing', '.'], ['a', 'null', 'hypothesis', ',', 'h0', 'is', 'con-', 'structed', 'to', 'model', 'the', 'situation', 'in', 'which', 'there', 'is', 'no', 'correlation', 'between', 'corpus', 'linguistics', 'and', 'linguistic', 'theory', '1⫺2', '(', '2005', ')', ',', '263⫺275', '1613-7027/05/0001⫺0263', '쑕', 'walter', 'de', 'gruyter', '264', 'a.', 'kilgarriff', 'the', 'two', 'phenomena', '.'], ['as', 'the', 'mathematics', 'of', 'the', 'random', 'is', 'well', 'under-', 'stood', ',', 'we', 'can', 'compute', 'the', 'likelihood', 'of', 'the', 'null', 'hypothesis', 'given', 'the', 'data', '.'], ['if', 'the', 'likelihood', 'is', 'low', ',', 'we', 'reject', 'h0', '.'], ['the', 'problem', 'for', 'empirical', 'linguistics', 'is', 'that', 'language', 'is', 'not', 'random', ',', 'so', 'the', 'null', 'hypothesis', 'is', 'never', 'true', '.'], ['language', 'is', 'not', 'random', 'because', 'we', 'speak', 'or', 'write', 'with', 'purposes', '.'], ['we', 'do', 'not', ',', 'indeed', ',', 'without', 'computational', 'help', 'are', 'not', 'capable', 'of', ',', 'producing', 'words', 'or', 'sounds', 'or', 'sentences', 'or', 'documents', 'randomly', '.'], ['we', 'do', 'not', 'always', 'have', 'enough', 'data', 'to', 'reject', 'the', 'null', 'hypothesis', ',', 'but', 'that', 'is', 'a', 'distinct', 'issue', ':', 'wherever', 'there', 'is', 'enough', 'data', ',', 'it', 'is', 'rejected', '.'], ['using', 'language', 'corpora', ',', 'we', 'are', 'frequently', 'in', 'the', 'fortunate', 'position', 'of', 'having', 'very', 'large', 'quantities', 'of', 'data', 'at', 'our', 'disposal', '.'], ['then', ',', 'even', 'where', 'pairs', 'of', 'corpora', 'are', 'set', 'up', 'to', 'be', 'linguistically', 'identical', ',', 'the', 'null', 'hypothesis', 'is', 'resoundingly', 'defeated', '.'], ['in', 'section', '4', ',', 'we', 'present', 'an', 'experiment', 'demonstrating', 'this', 'counterintuitive', 'effect', '.'], ['there', 'are', 'a', 'number', 'of', 'papers', 'in', 'the', 'empirical', 'linguistics', 'literature', 'where', 'researchers', 'seemed', 'to', 'be', 'testing', 'whether', 'an', 'association', 'was', 'lin-', 'guistically', 'salient', ',', 'or', 'used', 'the', 'confidence', 'with', 'which', 'h0', 'could', 'be', 're-', 'jected', 'as', 'a', 'measure', 'of', 'salience', ',', 'whereas', 'in', 'fact', 'they', 'were', 'merely', 'testing', 'whether', 'they', 'had', 'enough', 'data', 'to', 'reject', 'h0', 'with', 'confidence', '.'], ['some', 'such', 'cases', 'are', 'reviewed', 'in', 'section', '5', '.'], ['hypothesis', 'testing', 'has', 'been', 'widely', 'used', 'in', 'the', 'acquisition', 'of', 'subcategorization', 'frames', 'from', 'corpora', 'and', 'this', 'literature', 'is', 'considered', 'in', 'some', 'detail', '.'], ['alternatives', 'to', 'inappropriate', 'hy-', 'pothesis-testing', 'are', 'presented', '.'], ['before', 'proceeding', ',', 'may', 'i', 'clarify', 'that', 'this', 'paper', 'is', 'in', 'no', 'way', 'critical', 'of', 'using', 'probability', 'models', ',', 'all', 'of', 'which', 'are', 'based', 'on', 'assumptions', 'of', 'randomness', ',', 'in', 'empirical', 'linguistics', 'in', 'general', '.'], ['probability', 'models', 'have', 'been', 'responsible', 'for', 'a', 'large', 'share', 'of', 'progress', 'in', 'the', 'field', 'in', 'the', 'last', 'decade', 'and', 'a', 'half', '.'], ['the', 'randomness', 'assumptions', 'are', 'always', 'untrue', ',', 'but', 'that', 'does', 'not', 'preclude', 'them', 'from', 'frequently', 'being', 'useful', '.'], ['making', 'false', 'assumptions', 'is', 'often', 'an', 'ingenious', 'way', 'to', 'proceed', ';', 'the', 'problem', 'arises', 'where', 'the', 'literal', 'falsity', 'of', 'the', 'assumption', 'is', 'overlooked', ',', 'and', 'inappropri-', 'ate', 'inferences', 'are', 'drawn', '.'], ['2', '.'], ['the', 'arbitrary', 'and', 'the', 'random', 'in', 'common', 'parlance', ',', 'random', 'and', 'arbitrary', 'are', 'synonyms', ',', 'with', 'diction-', 'aries', 'giving', 'near-identical', 'definitions', ':', 'ldoce', '(', '1995', ')', 'defines', 'random', 'as', 'happening', 'or', 'chosen', 'without', 'any', 'definite', 'plan', ',', 'or', 'pattern', 'and', 'arbitrary', 'as', '1', 'decided', 'or', 'arranged', 'without', 'any', 'reason', 'or', 'plan', ',', 'often', 'unfairly', '…', '2', 'happening', 'or', 'decided', 'by', 'chance', 'rather', 'than', 'a', 'plan', 'language', 'is', 'never', ',', 'ever', ',', 'ever', ',', 'random', '265', 'superficially', ',', 'randomness', ',', 'as', 'defined', 'here', ',', 'is', 'what', 'the', 'technical', 'sense', 'of', 'random', 'captures', 'and', 'makes', 'explicit', '.'], ['the', 'technical', 'sense', 'is', 'defined', 'in', 'terms', 'of', 'statistical', 'independence', '.'], ['first', ',', 'we', 'formalize', 'the', 'framework', ':', 'for', 'a', 'population', 'of', 'events', ',', 'the', 'first', 'phenomenon', 'holds', 'where', 'x', 'is', 'true', 'of', 'the', 'event', ',', 'the', 'second', 'holds', 'where', 'y', 'is', 'true', 'of', 'the', 'event', '.'], ['now', ',', 'the', 'relation', 'between', 'the', 'phenomena', 'is', 'random', 'iff', 'the', 'prob-', 'ability', 'of', 'x', ',', 'for', 'that', 'subset', 'of', 'events', 'where', 'y', 'does', 'hold', ',', 'is', 'identical', 'to', 'its', 'probability', 'for', 'the', 'subset', 'where', 'y', 'does', 'not', 'hold', ',', 'that', 'is', 'p', '(', 'x|y', ')', '⫽', 'p', '(', 'x|ÿ', 'y', ')', 'the', 'relation', 'is', 'symmetric', ':', 'p', '(', 'x|y', ')', '⫽', 'p', '(', 'x|ÿ', 'y', ')', 'entails', 'p', '(', 'y|x', ')', '⫽', 'p', '(', 'y|ÿ', 'x', ')', '.'], ['hereafter', 'i', 'use', '‘', 'random', '’', 'for', 'the', 'technical', 'meaning', 'and', '‘', 'arbi-', 'trary', '’', 'for', 'the', 'non-technical', 'one', '.'], ['arbitrary', 'events', 'are', 'very', 'rarely', 'random', ',', 'and', 'random', 'events', 'are', 'very', 'rarely', 'arbitrary', '.'], ['it', 'takes', 'considerable', 'ingenuity', 'and', 'sophisticated', 'mathe-', 'matics', 'to', 'produce', 'a', 'pseudo-random', 'sequence', 'algorithmically', ',', 'and', 'true', 'randomness', 'is', 'not', 'possible', 'at', 'all', '.'], ['events', 'happening', '“', 'without', 'any', 'defi-', 'nite', 'plan', ',', 'aim', 'or', 'pattern', '”', 'are', ',', 'by', 'definition', ',', 'arbitrary', ',', 'but', 'are', 'vanish-', 'ingly', 'unlikely', 'to', 'be', 'random', '.'], ['outside', 'the', 'sub-atomic', 'realm', ',', 'natural', 'events', 'are', 'very', 'rarely', 'random', '.'], ['consider', ',', 'for', 'example', ',', 'cat', 'food', 'purchases', 'and', 'shoe-polish', 'purchases', 'within', 'the', 'space', 'of', 'all', 'uk', 'supermarket-shopping', 'events', ':', 'does', 'the', 'fact', 'that', 'cat', 'food', 'was', 'bought', 'predict', '(', 'positively', 'or', 'negatively', ')', 'whether', 'shoe', 'polish', 'was', 'bought', 'in', 'the', 'same', 'shopping', 'trip', '?'], ['there', 'is', 'no', 'obvious', 'reason', 'why', 'it', 'should', ',', 'and', 'we', 'can', 'happily', 'declare', 'the', 'relation', 'arbitrary', '.'], ['but', 'perhaps', 'either', 'cat', 'food', 'or', 'shoe-polish', 'are', 'more', '(', 'or', 'less', ')', 'often', 'bought', 'in', 'hot', '(', 'or', 'cold', ')', 'weather', ',', 'or', 'on', 'saturday', 'nights', ',', 'or', 'sunday', 'mornings', ',', 'or', 'monday', 'lunchtimes', ',', 'or', 'by', 'richer', '(', 'or', 'poorer', ')', 'people', ',', 'or', 'by', 'men', '(', 'or', 'women', ')', ',', 'or', 'by', 'people', 'in', '(', 'or', 'out', 'of', ')', 'towns…', 'there', 'is', 'an', 'unlimited', 'number', 'of', 'hypotheses', 'connecting', 'the', 'two', '(', 'positively', 'or', 'negatively', ')', ';', 'if', 'just', 'one', 'of', 'these', 'has', 'any', 'validity', ',', 'however', 'weak', ',', 'then', 'the', 'null', 'hypothesis', 'is', 'false', '.'], ['at', 'this', 'point', ',', 'you', 'may', 'question', 'why', 'the', 'null', 'hypothesis', 'is', 'ever', 'a', 'useful', 'construct', '.'], ['for', 'a', 'wide', 'range', 'of', 'tasks', ',', 'although', 'h0', 'is', 'false', ',', 'there', 'is', 'only', 'enough', 'evidence', 'to', 'establish', 'the', 'fact', 'if', 'there', 'is', 'a', 'strong', 'relation', 'between', 'the', 'two', 'phenomena', '.'], ['thus', ',', 'given', 'evidence', 'from', '1,000', 'shopping', 'trips', ',', 'it', 'is', 'un-', 'likely', 'we', 'shall', 'be', 'able', 'to', 'reject', 'h0', 'concerning', 'cat', 'food', 'and', 'shoe-polish', ',', 'whereas', 'we', 'shall', 'be', 'able', 'to', 'reject', 'it', 'concerning', 'strawberry-buying', 'and', 'cream-buying', '.'], ['given', 'further', 'evidence', ',', 'perhaps', 'from', '1,000,000', 'shopping', '266', 'a.', 'kilgarriff', 'trips', ',', 'we', 'shall', 'also', 'be', 'able', 'to', 'reject', 'the', 'null', 'hypothesis', 'regarding', 'nappy2-', 'buying', 'and', 'beer-sixpack-buying', '.'], ['(', 'the', 'correlation', ',', 'the', 'most', 'newsworthy', 'product', 'of', 'large-scale', 'data', 'mining', 'by', 'supermarkets', ',', 'was', 'widely', 'reported', 'in', 'the', 'british', 'media', '.', ')'], ['but', 'still', 'not', 'for', 'catf', 'ood', 'and', 'shoe-polish', '.'], ['but', ',', 'given', '1,000,000,000', 'events', ',', 'we', 'shall', 'in', 'all', 'likelihood', 'also', 'be', 'able', 'to', 'reject', 'it', 'for', 'cat', 'food', 'and', 'shoe-polish', '.'], ['whether', 'or', 'not', 'we', 'can', 'reject', 'the', 'null', 'hypothesis', '(', 'with', 'eg', '.'], ['95', '%', 'confi-', 'dence', ')', 'is', 'a', 'function', 'of', 'sample', 'size', 'and', 'level', 'of', 'correlation', '.'], ['where', 'sample', 'size', 'is', 'held', 'constant', '(', 'and', 'is', 'not', 'enormous', ')', ',', 'whether', 'or', 'not', 'we', 'can', 'reject', 'h0', 'can', 'be', 'seen', 'as', 'a', 'way', 'of', 'providing', 'statistical', 'support', 'for', 'distinguish-', 'ing', 'the', 'arbitrary', 'and', 'the', 'motivated', '.'], ['this', 'is', 'a', 'role', 'that', 'hypothesis', 'testing', 'plays', 'across', 'the', 'social', 'sciences', '.'], ['however', 'where', 'the', 'sample', 'size', 'varies', 'by', 'an', 'order', 'of', 'magnitude', ',', 'or', 'where', 'it', 'is', 'enormous', ',', 'it', 'is', 'wrong', 'to', 'identify', 'the', 'accept-h0/reject-h0', 'distinction', 'with', 'the', 'arbitrary/motivated', 'one', '.'], ['the', 'uneasy', 'relationship', 'between', 'hypothesis-testing', ',', 'and', 'quantity', 'of', 'data', ',', 'is', 'familiar', 'to', 'statisticians', 'though', 'frequently', 'overlooked', 'or', 'mis-', 'understood', 'by', 'users', 'of', 'statistics', '(', 'carver', '1993', ',', 'stubbs', '1995', ',', 'brandstätter', '1999', ')', '.'], ['one', 'statistics', 'textbook', 'warns', 'thus', ':', 'none', 'of', 'the', 'null', 'hypotheses', 'we', 'have', 'considered', 'with', 'respect', 'to', 'good-', 'ness', 'of', 'fit', 'can', 'be', 'exactly', 'true', ',', 'so', 'if', 'we', 'increase', 'the', 'sample', 'size', '(', 'and', 'hence', 'the', 'value', 'of', 'χ2', ')', 'we', 'would', 'ultimately', 'reach', 'the', 'point', 'when', 'all', 'null', 'hypotheses', 'would', 'be', 'rejected', '.'], ['all', 'that', 'the', 'χ2', 'test', 'can', 'tell', 'us', ',', 'then', ',', 'is', 'that', 'the', 'sample', 'size', 'is', 'too', 'small', 'to', 'reject', 'the', 'null', 'hypothesis', '!'], ['(', 'owen', 'and', 'jones', ',', '1977', ',', 'p', '359', ')', '.'], ['the', 'issue', 'is', 'particularly', 'salient', 'for', 'empirical', 'linguistics', 'because', ',', 'firstly', ',', 'we', 'have', 'access', 'to', 'extremely', 'large', 'sample', 'sizes', ',', 'and', 'secondly', ',', 'the', 'distri-', 'bution', 'of', 'many', 'language', 'phenomena', 'is', 'zipfian', '.'], ['the', 'has', '6,000,000', 'oc-', 'currences', 'in', 'the', 'bnc', 'whereas', 'cat', 'food', '(', 'spelled', 'as', 'one', 'word', 'or', 'two', ')', 'has', '66', '.'], ['for', 'a', 'vast', 'number', 'of', 'third', 'phenomena', 'x', ',', 'the', 'null', 'hypothesis', 'that', 'the', 'and', 'x', 'are', 'uncorrelated', 'will', 'be', 'rejected', ',', 'whereas', 'the', 'null', 'hypothesis', 'that', 'cat', 'food', 'and', 'x', 'are', 'uncorrelated', 'will', 'not', '.'], ['it', 'would', 'be', 'wrong', 'to', 'draw', 'inferences', 'about', 'what', 'was', 'arbitrary', ',', 'what', 'motivated', '.'], ['3', '.'], ['objections', 'to', 'maximum', 'likelihood', 'estimates', '(', 'mles', ')', 'church', 'and', 'hanks', '(', '1990', ')', 'inaugurated', 'the', 'research', 'area', 'of', 'lexical', 'statis-', 'tics', 'with', 'their', 'presentation', 'of', 'mutual', 'information', '(', 'i', ')', ',', 'a', 'measure', 'of', 'how', 'closely', 'associated', 'two', 'phenomena', 'are', '.'], ['it', 'can', 'be', 'applied', 'to', 'finding', 'words', 'which', 'occur', 'together', 'to', 'a', 'noteworthy', 'degree', ',', 'or', 'to', 'finding', 'words', 'which', 'are', 'particularly', 'associated', 'with', 'one', 'corpus', 'as', 'against', 'another', ',', 'or', 'for', 'various', 'other', 'purposes.3', 'they', 'define', 'the', 'mutual', 'information', 'between', 'two', 'words', 'x', 'and', 'y', 'as', 'language', 'is', 'never', ',', 'ever', ',', 'ever', ',', 'random', '267', 'i', '(', 'x', ';', 'y', ')', '⫽', 'log', '2', '冉', 'p', '(', 'xandy', ')', 'p', '(', 'x', ')', '·', 'p', '(', 'y', ')', '冊', 'and', 'then', 'estimate', 'probabilities', 'directly', 'from', 'frequencies', ',', 'that', 'is', 'using', 'the', '‘', 'maximum', 'likelihood', 'estimate', '’', '(', 'mle', ')', 'of', 'f', '(', 'x', ')', '/n', 'for', 'p', '(', 'x', ')', ',', 'f', '(', 'y', ')', '/n', 'for', 'p', '(', 'y', ')', ',', 'f', '(', 'x-and-y', ')', '/n', 'for', 'p', '(', 'x-and-y', ')', ',', 'thereby', 'giving', 'i', '(', 'x', ';', 'y', ')', '⫽', 'log', '2', '冉', 'n', '·', 'f', '(', 'x', '⫺', 'and', '⫺', 'y', ')', 'f', '(', 'x', ')', '·', 'f', '(', 'y', ')', '冊', 'dunning', '(', '1993', ')', 'presents', 'a', 'critique', 'of', 'the', 'use', 'of', 'mutual', 'information', 'in', 'empirical', 'linguistics', '.'], ['his', 'objection', 'has', 'been', 'confused', 'with', 'the', 'critique', 'of', 'hypothesis-testing', 'i', 'make', 'here', 'so', 'i', 'mention', 'his', 'work', 'in', 'order', 'to', 'clarify', 'that', 'the', 'two', 'objections', ',', 'while', 'both', 'valid', ',', 'are', 'different', 'in', 'nature', 'and', 'independent', '.'], ['dunning', 'demonstrates', 'how', 'mles', 'fare', 'poorly', 'when', 'estimating', 'the', 'probabilities', 'of', 'rare', 'events', '.'], ['the', 'problem', 'is', 'essentially', 'this', ':', 'if', 'a', 'word', '(', 'or', 'bigram', ',', 'or', 'trigram', ',', 'or', 'character-sequence', 'etc', '.', ')'], ['occurs', 'just', 'once', 'or', 'twice', 'in', 'a', 'corpus', 'of', 'n', 'words', '(', 'bigrams', ',', 'etc', '.'], [')', ',', 'then', 'the', 'simplest', 'way', 'to', 'estimate', 'the', 'probability', 'is', 'the', 'nile', ',', 'which', 'gives', '1/n', 'or', '2/n', '.'], ['however', 'this', 'does', 'not', 'factor', 'in', 'the', 'arbitrariness', 'of', 'the', 'word', 'occurring', 'at', 'all', 'in', 'the', 'corpus', ':', 'in', 'a', 'corpus', 'ten', 'times', 'the', 'size', ',', 'there', 'would', 'be', 'roughly', 'ten', 'times', 'the', 'number', 'of', 'singletons', 'and', 'doubletons', 'in', 'the', 'corpus', ',', 'most', 'of', 'which', 'would', 'not', 'have', 'occurred', 'at', 'all', 'in', 'the', 'original', 'corpus', '.'], ['thus', 'some', 'of', 'the', 'prob-', 'ability', 'mass', 'contributing', 'to', 'the', '1/n', 'or', '2/n', 'mles', 'should', 'have', 'been', 'put', 'aside', 'for', 'the', 'words', '(', 'bigrams', 'etc', '.', ')'], ['which', 'did', 'not', 'occur', 'at', 'all', 'in', 'this', 'particular', 'corpus', '.'], ['viewed', 'another', 'way', ',', 'the', '1/n', 'and', '2/n', 'should', 'be', 'dis-', 'counted', 'to', 'allow', 'for', 'the', 'fact', 'that', 'one', 'or', 'two', 'occurrences', 'are', 'very', 'low', 'bases', 'of', 'evidence', 'on', 'which', 'to', 'assert', 'probabilities', '.'], ['there', 'are', 'various', 'ways', 'in', 'which', 'the', 'discounting', 'can', 'be', 'done', ',', 'for', 'example', 'the', 'good-turing', 'method', '(', 'good', '1953', ')', ',', 'usefully', 'applied', 'to', 'em-', 'pirical', 'linguistics', 'in', 'gale', 'and', 'sampson', '(', '1995', ')', ',', 'bod', '(', '1995', ')', '.'], ['dunning', 'presents', 'and', 'advocates', 'the', 'use', 'of', 'the', 'log-likelihood', 'statistic', ',', 'which', ',', 'like', 'the', 'χ2', 'statistic', ',', 'is', 'χ2-distributed,4', 'but', 'more', 'accurately', 'estimates', 'probabil-', 'ities', 'where', 'counts', 'are', 'low', '.'], ['the', 'log-likelihood', 'statistic', 'still', 'only', 'estimates', 'probabilities', ':', 'since', 'dunning', '’', 's', 'work', ',', 'pedersen', '(', '1996', ')', 'has', 'shown', 'how', 'fisher', '’', 's', 'exact', 'method', 'can', 'be', 'applied', 'to', 'the', 'problem', ',', 'to', 'identify', 'the', 'exact', 'probability', 'of', 'a', 'word', '(', 'bigram', 'etc', '.', ')'], ['rather', 'than', 'estimating', 'it', 'at', 'all', '.'], ['thus', 'dunning', '’', 's', 'objection', 'to', 'mutual', 'information', 'is', 'that', 'it', 'fails', 'to', 'accurately', 'represent', 'probabilities', 'when', 'counts', 'are', 'low', '(', 'where', '‘', 'low', '’', 'is', 'generally', 'taken', 'as', 'less', 'than', 'five', ')', '.'], ['if', 'the', 'probabilities', 'can', 'be', 'accurately', 'represented', ',', 'dunning', '’', 's', 'anxieties', 'will', 'be', 'set', 'at', 'ease', '.'], ['268', 'a.', 'kilgarriff', 'the', 'critique', 'in', 'this', 'paper', 'does', 'not', 'concern', 'whether', 'probabilities', 'are', 'accurately', 'calculated', '.'], ['rather', ',', 'the', 'objection', 'is', 'that', 'the', 'probability', 'model', ',', 'with', 'its', 'assumptions', 'of', 'randomness', ',', 'is', 'inappropriate', ',', 'particularly', 'where', 'counts', 'are', 'high', '(', 'eg', ',', 'thousands', 'or', 'more', ')', '.'], ['where', 'the', 'task', 'is', 'to', 'determine', 'whether', 'there', 'is', 'an', 'interesting', 'associa-', 'tion', 'between', 'two', 'rare', 'events', ',', 'dunning', '’', 's', 'concern', 'must', 'be', 'heeded', '.'], ['where', 'it', 'is', 'to', 'determine', 'whether', 'there', 'is', 'an', 'interesting', 'association', 'between', 'high-frequency', 'events', ',', 'the', 'concerns', 'of', 'this', 'paper', 'must', 'be', '.'], ['4', '.'], ['experiment', 'given', 'enough', 'data', ',', 'h0', 'is', 'almost', 'always', 'rejected', 'however', 'arbitrary', 'the', 'data', ',', 'as', 'the', 'author', 'discovered', 'when', 'grappling', 'with', 'the', 'following', 'data', '.'], ['two', 'corpora', 'were', 'set', 'up', 'to', 'be', 'indisputably', 'of', 'the', 'same', 'language', 'type', ',', 'with', 'only', 'arbitrary', 'differences', 'between', 'them', ':', 'each', 'was', 'a', 'random', 'subset', 'of', 'the', 'written', 'part', 'of', 'the', 'british', 'national', 'corpus', '(', 'bnc', ')', '.'], ['the', 'sampling', 'was', 'as', 'follows', ':', 'all', 'texts', 'shorter', 'than', '20,000', 'words', 'were', 'excluded', '.'], ['this', 'left', '820', 'texts', '.'], ['half', 'the', 'texts', 'were', 'then', 'randomly', 'assigned', 'to', 'each', 'of', 'two', 'corpora', '.'], ['the', 'null', 'hypotheses', 'were', '(', '1', ')', 'that', 'the', 'two', 'subcorpora', ',', 'viewed', 'as', 'col-', 'lections', 'of', 'words', 'rather', 'than', 'documents', ',', 'were', 'random', 'samples', 'drawn', 'from', 'the', 'same', 'population', ';', 'and', 'consequently', ',', '(', '2', ')', 'that', 'the', 'deviation', 'in', 'frequency', 'of', 'occurrence', 'for', 'each', 'individual', 'word', 'between', 'the', 'two', 'sub-', 'corpora', 'was', 'explicable', 'as', 'random', 'fluctuation', '.'], ['the', 'ho', 'were', 'tested', 'using', 'the', 'χ2-test', ':', 'is', 'χ2', 'σ', '(', '|o', '⫺', 'e', '|', '⫺', '0.5', ')', '2', '/e', 'greater', 'than', 'the', 'critical', 'value', '?'], ['the', 'sum', 'is', 'over', 'the', 'four', 'cells', 'of', 'the', 'contingency', 'table', 'corpus', '1', 'corpus', '2', 'word', 'w', 'a', 'b', 'not', 'word', 'w', 'c', 'd', 'if', 'we', 'randomly', 'assign', 'words', '(', 'as', 'opposed', 'to', 'documents', ')', 'to', 'the', 'one', 'corpus', 'or', 'the', 'other', ',', 'then', 'we', 'have', 'a', 'straightforward', 'random', 'distribution', ',', 'with', 'the', 'value', 'of', 'the', 'χ2-statistic', 'equal', 'to', 'or', 'greater', 'than', 'the', '99.5', '%', 'confidence', 'threshold', 'of', '7.88', 'for', 'just', '0.5', '%', 'of', 'words', '.'], ['the', 'average', 'value', 'of', 'the', 'error', 'term', ',', 'language', 'is', 'never', ',', 'ever', ',', 'ever', ',', 'random', '269', '(', '|o', '⫺', 'e|', '⫺', '0.5', ')', '2', '/e', 'is', 'then', '0.5.5', 'the', 'hypothesis', 'can', ',', 'therefore', ',', 'be', 'couched', 'as', ':', 'are', 'the', 'error', 'terms', 'systematically', 'greater', 'than', '0.5', '?'], ['if', 'they', 'are', ',', 'we', 'should', 'be', 'wary', 'of', 'attributing', 'high', 'error', 'terms', 'to', 'significant', 'differences', 'between', 'text', 'types', ',', 'since', 'we', 'also', 'obtain', 'many', 'high', 'error', 'terms', 'where', 'there', 'are', 'no', 'significant', 'differences', 'between', 'text', 'types', '.'], ['frequency', 'lists', 'for', 'word-pos', 'pairs', 'for', 'each', 'subcorpus', 'were', 'gener-', 'ated', '.'], ['for', 'each', 'word', 'occurring', 'in', 'either', 'subcorpus', ',', 'the', 'error', 'term', 'which', 'would', 'have', 'contributed', 'to', 'a', 'χ2', 'calculation', 'was', 'determined', '.'], ['as', 'table', '4', 'shows', ',', 'average', 'values', 'for', 'the', 'error', 'term', 'are', 'far', 'greater', 'than', '0.5', ',', 'and', 'tend', 'to', 'increase', 'as', 'word', 'frequency', 'increases', '.'], ['as', 'the', 'averages', 'indicate', ',', 'the', 'error', 'term', 'is', 'very', 'often', 'greater', 'than', '0.5', '*', '7.88', '⫽', '3.94', ',', 'the', 'relevant', 'critical', 'value', 'of', 'the', 'chi-square', 'statistic', '.'], ['for', 'very', 'many', 'words', ',', 'including', 'most', 'common', 'words', ',', 'the', 'null', 'hypothesis', 'is', 'resoundingly', 'defeated', '(', 'as', 'is', 'the', 'null', 'hypthesis', 'regarding', 'the', 'two', 'subc-', 'orpora', 'as', 'wholes', ')', '.'], ['there', 'is', 'no', 'a', 'priori', 'reason', 'to', 'expect', 'words', 'to', 'behave', 'as', 'if', 'they', 'had', 'been', 'selected', 'at', 'random', ',', 'and', 'indeed', 'they', 'do', 'not', '.'], ['it', 'is', 'in', 'the', 'nature', 'of', 'table', '1', '.'], ['comparing', 'two', 'same-genre', 'corpora', 'using', 'χ2', 'class', 'first', 'item', 'in', 'class', 'mean', 'error', 'term', '(', 'words', 'in', 'freq', '.'], ['order', ')', 'for', 'items', 'in', 'class', 'word', 'pos', 'first', '10', 'items', 'the', 'det', '18.76', 'next', '10', 'items', 'for', 'prp', '17.45', 'next', '20', 'items', 'not', 'xx', '14.39', 'next', '40', 'items', 'have', 'vhb', '10.71', 'next', '80', 'items', 'also', 'avo', '7.03', 'next', '160', 'items', 'know', 'vvi', '6.40', 'next', '320', 'items', 'six', 'crd', '5.30', 'next', '640', 'items', 'finally', 'av0', '6.71', 'next', '1280', 'items', 'plants', 'nn2', '6.05', 'next', '2560', 'items', 'pocket', 'nn1', '5.82', 'next', '5120', 'items', 'represent', 'vvb', '4.53', 'next', '10240', 'items', 'peking', 'np0', '3.07', 'next', '20480', 'items', 'fondly', 'av0', '1.87', 'next', '40960', 'items', 'chandelier', 'nn1', '1.15', 'table', 'note', '.'], ['mean', 'error', 'term', 'is', 'far', 'greater', 'than', '0.5', ',', 'and', 'increases', 'with', 'frequency', '.'], ['pos', 'tags', 'are', 'drawn', 'from', 'the', 'claws-5', 'tagset', 'as', 'used', 'in', 'the', 'bnc', '(', 'see', 'http', ':', '/info.ox', '.'], ['ac.uk/bnc', ')', '270', 'a.', 'kilgarriff', 'language', 'that', 'any', 'two', 'collections', 'of', 'texts', ',', 'covering', 'a', 'wide', 'range', 'of', 'registers', '(', 'and', 'comprising', ',', 'say', ',', 'less', 'than', 'a', 'thousand', 'samples', 'of', 'over', 'a', 'thousand', 'words', 'each', ')', 'will', 'show', 'such', 'differences', '.'], ['while', 'it', 'might', 'seem', 'plausible', 'that', 'oddities', 'would', 'in', 'some', 'way', 'balance', 'out', 'to', 'give', 'a', 'popula-', 'tion', 'that', 'was', 'indistinguishable', 'from', 'one', 'where', 'the', 'individual', 'words', '(', 'as', 'opposed', 'to', 'the', 'texts', ')', 'had', 'been', 'randomly', 'selected', ',', 'this', 'turns', 'out', 'not', 'to', 'be', 'the', 'case', '.'], ['the', 'key', 'word', 'in', 'the', 'last', 'paragraph', 'is', '‘', 'indistinguishable', '’', '.'], ['in', 'hypothesis', 'testing', ',', 'the', 'objective', 'is', 'generally', 'to', 'see', 'if', 'the', 'population', 'can', 'be', 'distin-', 'guished', 'from', 'one', 'that', 'has', 'been', 'randomly', 'generated', '⫺', 'or', ',', 'in', 'our', 'case', ',', 'to', 'see', 'if', 'the', 'two', 'populations', 'are', 'distinguishable', 'from', 'two', 'populations', 'which', 'have', 'been', 'randomly', 'generated', 'on', 'the', 'basis', 'of', 'the', 'frequencies', 'in', 'the', 'joint', 'corpus', '.'], ['since', 'words', 'in', 'a', 'text', 'are', 'not', 'random', ',', 'we', 'know', 'that', 'our', 'corpora', 'are', 'not', 'randomly', 'generated', ',', 'and', 'the', 'hypothesis', 'test', 'con-', 'firms', 'the', 'fact', '.'], ['5', '.'], ['re-analysis', 'of', 'previous', 'work', '5.1', '.'], ['brown', 'and', 'lob', 'hofland', 'and', 'johansson', '(', '1982', ')', 'wanted', 'to', 'find', 'words', 'which', 'were', 'signifi-', 'cantly', 'different', 'in', 'their', 'frequencies', 'between', 'british', 'and', 'american', 'eng-', 'lish', ',', 'as', 'represented', 'in', 'the', 'brown', 'corpus', 'for', 'american', 'english', 'and', 'lob', 'corpus', 'for', 'british', '.'], ['for', 'each', 'word', ',', 'they', 'tested', 'the', 'null', 'hypothesis', 'that', 'the', 'difference', 'in', 'frequency', 'between', 'the', 'two', 'corpora', 'could', 'be', 'explained', 'as', 'random', 'variation', ',', 'with', 'the', 'samples', 'being', 'random', 'samples', 'from', 'the', 'same', 'source', ',', 'and', 'in', 'their', 'frequency', 'lists', ',', 'they', 'mark', 'words', 'where', 'the', 'null', 'hypothesis', 'was', 'defeated', '(', 'at', 'a', '95', ',', '99', 'or', '99.9', '%', 'confidence', 'level', ')', '.'], ['looking', 'at', 'these', 'lists', 'suggests', 'that', 'virtually', 'all', 'common', 'words', 'are', 'markedly', 'different', 'in', 'their', 'levels', 'of', 'use', 'between', 'the', 'us', 'and', 'the', 'uk', ':', 'they', 'are', 'all', 'marked', 'as', 'such', '.'], ['by', 'contrast', ',', 'most', 'of', 'the', 'rarer', 'marked', 'words', 'are', 'words', 'we', 'know', 'to', 'be', 'american', 'or', 'british', ',', 'or', 'to', 'refer', 'to', 'items', 'that', 'are', 'more', 'common', 'or', 'more', 'salient', 'in', 'the', 'us', 'or', 'the', 'uk', '.'], ['as', 'the', 'argument', 'of', 'the', 'previous', 'section', 'explains', ',', 'most', 'of', 'the', 'marked', 'high-frequency', 'words', 'are', 'marked', 'simply', 'as', 'a', 'consequence', 'of', 'the', 'essen-', 'tially', 'non-random', 'nature', 'of', 'language', '.'], ['it', 'would', 'not', 'be', 'surprising', 'for', 'a', 'high-frequency', 'word', 'marked', 'as', 'british', 'english', 'in', 'these', 'lists', 'to', 'be', 'marked', 'as', 'american', 'english', 'in', 'a', 'repeat', 'of', 'the', 'experiment', 'using', 'new', 'data', '.'], ['similar', 'strategies', 'are', 'used', 'by', ',', 'and', 'a', 'similar', 'critique', 'is', 'applicable', 'to', ',', 'leech', 'and', 'fallon', '(', '1992', ')', '(', 'again', ',', 'for', 'comparing', 'lob', 'and', 'brown', ')', ',', 'ray-', 'language', 'is', 'never', ',', 'ever', ',', 'ever', ',', 'random', '271', 'son', ',', 'leech', ',', 'and', 'hodges', '(', '1997', ')', 'for', 'comparing', 'the', 'conversation', 'of', 'dif-', 'ferent', 'social', 'groups', ',', 'and', 'rayson', 'and', 'garside', '(', '2000', ')', 'for', 'contrasting', 'the', 'language', 'of', 'a', 'specialist', 'genre', 'with', '‘', 'general', 'language', '’', ',', 'as', 'represented', 'by', 'the', 'british', 'national', 'corpus', '.'], ['5.2', 'subcategorization', 'frame', '(', 'scf', ')', 'learning', 'hypothesis-testing', 'has', 'been', 'used', 'in', 'a', 'number', 'of', 'papers', 'concerning', 'the', 'automatic', 'acquisition', 'of', 'subcategorization', 'frames', '(', 'scfs', ')', 'for', 'verbs', 'from', 'corpora', '.'], ['the', 'problem', 'is', 'this', '.'], ['dictionaries', ',', 'even', 'where', 'they', 'do', 'present', 'explicit', 'and', 'accurate', 'scfs', 'for', 'verbs', ',', 'are', 'not', 'complete', ':', 'they', 'do', 'not', 'pre-', 'sent', 'all', 'the', 'frames', 'for', 'each', 'verb', '.'], ['this', 'gives', 'rise', 'to', 'many', 'parsing', 'errors', '.'], ['researchers', 'including', 'brent', '(', '1993', ')', ',', 'briscoe', 'and', 'carroll', '(', '1997', ')', 'and', 'kor-', 'honen', '(', '2000', ')', 'have', 'developed', 'methods', 'for', 'scf', 'acquisition', '.'], ['however', ',', 'their', 'methods', 'are', 'inevitably', 'noisy', ',', 'suffering', ',', 'for', 'example', ',', 'from', 'just', 'those', 'parser', 'errors', 'that', 'the', 'whole', 'process', 'is', 'designed', 'to', 'address', ',', 'and', 'they', 'do', 'not', 'wish', 'to', 'accept', 'any', 'scf', 'for', 'which', 'there', 'is', 'any', 'evidence', 'as', 'a', 'true', 'scf', 'for', 'the', 'verb', '.'], ['they', 'wish', 'to', 'filter', 'out', 'those', 'scfs', 'where', 'the', 'evidence', 'is', 'not', 'strong', 'enough', '.'], ['brent', 'and', 'briscoe', 'and', 'carroll', 'used', 'hypothesis', 'testing', 'to', 'this', 'end', '.'], ['however', ',', 'problems', 'are', 'noted', ':', 'further', 'evaluation', 'of', 'the', 'results', '...', 'reveals', 'that', 'the', 'filtering', 'phase', 'is', 'the', 'weak', 'link', 'in', 'the', 'system', '…', 'the', 'performance', 'of', 'the', 'filter', 'for', 'classes', 'with', 'less', 'than', '10', 'exemplars', 'is', 'around', 'chance', ',', 'and', 'a', 'simple', 'heuristic', 'of', 'accepting', 'all', 'classes', 'with', 'more', 'then', '10', 'exemplars', 'would', 'have', 'pro-', 'duced', 'broadly', 'similar', 'results', 'for', 'these', 'verbs', '(', 'briscoe', 'and', 'carroll', '1997', ':', '360⫺36', ')', '.'], ['korhonen', ',', 'correll', ',', 'and', 'mccarthy', '(', '2000', ')', 'explore', 'the', 'issue', 'in', 'detail', '.'], ['using', 'briscoe', 'and', 'carroll', '’', 's', 'scf', 'acquisition', 'system', ',', 'they', 'explore', 'the', 'impact', 'of', 'four', 'different', 'strategies', 'for', 'filtering', 'out', 'noise', ':', 'baseline', 'no', 'filter', 'bht', 'binomial', 'hypothesis', 'test', ':', 'reject', 'the', 'scf', 'if', 'ho', 'is', 'not', 'defeated6', 'llr', 'hypothesis', 'test', 'using', 'log-likelihood', 'ratio', ':', 'reject', 'the', 'scf', 'if', 'ho', 'is', 'not', 'defeated', 'mle', 'threshold', 'based', 'on', 'the', 'relative', 'frequency', '(', 'which', 'is', 'also', 'the', 'maximum', 'likelihood', 'estimate', '(', 'mle', ')', 'of', 'the', 'probability', ')', 'of', 'the', 'verb', 'occurring', 'in', 'the', 'scf', 'given', 'the', 'verb', ',', 'with', 'the', 'thresh-', 'old', 'determined', 'empirically', '272', 'a.', 'kilgarriff', 'they', 'observe', 'mle', 'thresholding', 'produced', 'better', 'results', 'than', 'the', 'two', 'statistical', 'tests', 'used', '.'], ['precision', 'improved', 'considerably', ',', 'showing', 'that', 'the', 'classes', 'occur-', 'ring', 'in', 'the', 'data', 'with', 'the', 'highest', 'frequency', 'are', 'often', 'correct', '…', 'mle', 'is', 'not', 'adept', 'at', 'finding', 'low', 'frequency', 'scfs', '…', '(', 'korhonen', ',', 'correll', ',', 'and', 'mccarthy', '2000', ':', '202', ')', 'this', 'concurs', 'with', 'the', 'theoretical', 'argument', 'above', '.'], ['hypothesis', 'tests', 'are', 'inappropriate', 'for', 'the', 'task', ',', 'because', 'the', 'relations', 'between', 'verb', 'and', 'scf', 'will', 'never', 'be', 'random', 'and', 'the', 'hypothesis', 'test', 'will', 'merely', 'reject', 'the', 'null', 'hypothesis', 'wherever', 'there', 'is', 'enough', 'data', ',', 'in', 'a', 'manner', 'not', 'closely', 'corre-', 'lated', 'with', 'whether', 'the', 'scf-verb', 'link', 'is', 'motivated', '.'], ['where', 'there', 'is', 'enough', 'data', ',', 'then', 'the', 'relationship', 'between', 'verb', 'and', 'scf', 'is', 'easy', 'to', 'see', 'so', 'even', 'a', 'simple', 'threshold', 'method', 'will', 'identify', 'the', 'verb', '’', 's', 'scfs', '.'], ['where', 'data', 'is', 'very', 'sparse', ',', 'no', 'method', 'works', 'well', '.'], ['korhonen', '(', '2000', ')', 'extends', 'this', 'line', 'of', 'work', ',', 'exploring', 'thresholding', 'methods', 'where', 'a', 'more', 'accurate', 'estimate', 'of', 'the', 'probability', 'is', 'obtained', 'by', 'using', 'data', 'from', 'semantically', 'similar', 'but', 'higher', 'frequency', 'verbs', '.'], ['she', 'achieves', 'modest', 'improvements', 'over', 'the', 'baseline', 'which', 'uses', 'korhonen', ',', 'correll', 'and', 'mccarthy', '’', 's', 'mle', ',', 'particularly', 'when', 'combining', 'the', 'fre-', 'quencies', 'of', 'the', 'target', 'verb', 'and', 'its', 'semantic', 'neighbour', 'using', 'a', 'linear', 'method', 'based', 'on', 'the', 'quantity', 'of', 'evidence', 'available', 'for', 'each', '.'], ['the', 'problem', 'is', 'not', 'one', 'of', 'distinguishing', 'random', 'and', 'non-random', 'relationships', ',', 'but', 'of', 'sparseness', 'of', 'data', '.'], ['where', 'the', 'data', 'is', 'not', 'sparse', ',', 'the', 'difference', 'between', 'arbitrary', 'and', 'motivated', 'connections', 'is', 'evident', 'in', 'greatly', 'differing', 'relative', 'frequencies', '.'], ['this', 'makes', 'the', 'moral', 'of', 'the', 'story', 'plain', '.'], ['data', 'is', 'abundant', '.'], ['a', 'modest-frequency', 'verb', 'like', 'devastate', 'occurs', '(', 'google', 'tells', 'us', ')', 'in', 'well', 'over', 'a', 'million', 'web', 'pages', '.'], ['with', 'just', '1', '%', 'of', 'them', ',', 'devastate', 'becomes', 'one', 'of', 'the', 'verbs', 'for', 'which', 'we', 'have', 'plenty', 'of', 'data', ',', 'and', 'crude', 'thresholding', 'methods', 'will', 'distinguish', 'associated', 'scfs', 'from', 'noise', '.'], ['it', 'is', 'possible', 'that', 'parsing', 'errors', 'are', 'systematic', 'and', 'thus', 'that', 'the', 'same', 'errors', 'occur', 'very', 'often', 'in', 'very', 'large', 'corpora', 'although', 'our', 'experi-', 'ence', 'from', 'looking', 'at', 'large', 'corpora', 'in', 'the', 'word', 'sketch', 'engine', '(', 'kilgarriff', 'et', 'al', '2004', ')', 'suggests', 'not', '.'], ['harvesting', 'the', 'web', '(', 'or', 'other', 'huge', 'corpora', ')', 'is', 'the', 'way', 'to', 'build', 'an', 'accurate', 'scf', 'lexicon.7', '6', '.'], ['conclusion', 'language', 'is', 'non-random', 'and', 'hence', ',', 'when', 'we', 'look', 'at', 'linguistic', 'phenom-', 'ena', 'in', 'corpora', ',', 'the', 'null', 'hypothesis', 'will', 'never', 'be', 'true', '.'], ['moreover', ',', 'where', 'there', 'is', 'enough', 'data', ',', 'we', 'shall', '(', 'almost', ')', 'always', 'be', 'able', 'to', 'establish', 'that', 'it', 'is', 'not', 'true', '.'], ['in', 'corpus', 'studies', ',', 'we', 'frequently', 'do', 'have', 'enough', 'data', ',', 'so', 'language', 'is', 'never', ',', 'ever', ',', 'ever', ',', 'random', '273', 'the', 'fact', 'that', 'a', 'relation', 'between', 'two', 'phenomena', 'is', 'demonstrably', 'non-', 'random', ',', 'does', 'not', 'support', 'the', 'inference', 'that', 'it', 'is', 'not', 'arbitrary', '.'], ['hypoth-', 'esis', 'testing', 'is', 'rarely', 'useful', 'for', 'distinguishing', 'associated', 'from', 'non-associ-', 'ated', 'pairs', 'of', 'phenomena', 'in', 'large', 'corpora', '.'], ['where', 'used', ',', 'it', 'has', 'often', 'led', 'to', 'unhelpful', 'or', 'misleading', 'results', '.'], ['hypothesis', 'testing', 'has', 'been', 'used', 'to', 'reach', 'conclusions', ',', 'where', 'the', 'diffi-', 'culty', 'in', 'reaching', 'a', 'conclusion', 'is', 'caused', 'by', 'sparsity', 'of', 'data', '.'], ['but', 'language', 'data', ',', 'in', 'this', 'age', 'of', 'information', 'glut', ',', 'is', 'available', 'in', 'vast', 'quantities', '.'], ['a', 'better', 'strategy', 'will', 'generally', 'be', 'to', 'use', 'more', 'data', 'then', 'the', 'difference', 'between', 'the', 'motivated', 'and', 'the', 'arbitrary', 'will', 'be', 'evident', 'without', 'the', 'use', 'of', 'compromised', 'hypothesis', 'testing', '.'], ['as', 'lord', 'rutherford', 'put', 'it', ':', '“', 'if', 'your', 'experiment', 'needs', 'statistics', ',', 'you', 'ought', 'to', 'have', 'done', 'a', 'better', 'experi-', 'ment.', '”', 'received', 'july', '2004', 'lexical', 'computing', 'ltd', '.'], ['revisions', 'received', 'may', '2005', 'final', 'acceptance', 'may', '2005', 'notes', '*', 'this', 'work', 'was', 'supported', 'by', 'the', 'uk', 'epsrc', ',', 'under', 'grants', 'gr/k18931', '(', 'seal', ')', 'and', 'gr/m54971', '(', 'wasps', ')', '.'], ['1', '.'], ['in', 'this', 'paper', 'we', 'do', 'not', 'consider', 'the', 'distinction', 'between', 'the', 'predictable', 'and', 'the', '‘', 'merely', '’', 'motivated', '.'], ['2', '.'], ['diapers', ',', 'in', 'american', 'english', '3', '.'], ['there', 'is', 'some', 'confusion', 'over', 'names', '.'], ['in', 'information', 'theory', ',', 'mutual', 'information', 'is', 'usually', 'defined', 'over', 'a', 'whole', 'population', 'of', 'words', ',', 'rather', 'than', 'being', 'specified', 'for', 'a', 'particular', 'word-pair', ',', 'as', 'here', ',', 'and', 'the', 'definition', 'incorporates', 'information', 'from', 'all', 'cells', 'of', 'the', 'contingency', 'table', '.'], ['church', 'and', 'hanks', 'only', 'use', 'a', 'subset', 'of', 'that', 'information', '.'], ['church-and-hanks', 'mutual', 'information', 'has', 'been', 'called', 'pointwise', 'mutual', 'information', '.'], ['see', 'manning', 'and', 'schütze', '(', '1999', ':', '66', 'ff', '.', ')'], ['for', 'a', 'fuller', 'discus-', 'sion', '.'], ['here', 'we', 'use', 'church', 'and', 'hanks', '’', 's', 'definition', 'and', 'name', '.'], ['4', '.'], ['this', 'sentence', 'will', 'be', 'confusing', 'to', 'non-mathematicians', '.'], ['the', 'χ2', 'statistic', 'is', 'a', 'statis-', 'tic', ',', 'that', 'is', ',', 'it', 'can', 'be', 'calculated', 'from', 'a', 'data', 'sample', 'using', 'actual', 'numbers', '.'], ['the', 'χ2', 'distribution', 'is', 'a', 'theoretical', 'construct', '.'], ['if', 'a', 'sufficiently', 'large', 'number', 'of', 'chi-square', 'statistics', 'are', 'calculated', ',', 'all', 'from', 'true', 'random', 'samples', 'of', 'the', 'same', 'population', ',', 'then', 'this', 'population', 'of', 'χ2', 'statistics', 'will', ',', 'provably', ',', 'fit', 'a', 'χ2', 'distribution', '.'], ['this', 'is', 'also', 'true', 'for', 'other', 'statistics', ':', 'that', 'is', ',', 'if', 'a', 'sufficiently', 'large', 'number', 'of', 'log-likelihood', 'statistics', 'are', 'calculated', ',', 'all', 'from', 'true', 'random', 'samples', 'of', 'the', 'same', 'population', ',', 'then', 'this', 'population', 'of', 'log-likelihood', 'statistics', 'will', ',', 'provably', ',', 'fit', 'a', 'χ2', 'distribution', '.'], ['some', 'texts', 'call', 'the', 'statistic', 'χ2', 'rather', 'than', 'χ2', 'to', 'distinguish', 'it', 'more', 'clearly', 'from', 'the', 'distribution', ',', 'but', 'this', 'practice', 'is', 'in', 'the', 'minority', 'and', 'is', 'not', 'adopted', 'here', '.'], ['5', '.'], ['see', 'appendix', '6', '.'], ['the', 'model', 'used', 'was', 'a', 'sophisticated', 'one', 'incorporating', 'evidence', 'about', 'type', 'fre-', 'quencies', 'of', 'verbs', 'from', 'the', 'anlt', 'lexicon', ':', 'see', 'briscoe', 'and', 'carroll', '(', '1997', ')', 'or', 'kor-', 'honen', ',', 'correll', ',', 'and', 'mccarthy', '(', '2000', ')', 'for', 'details', '.'], ['7', '.'], ['see', 'kilgarriff', 'and', 'grefenstette', '(', '2003', ')', 'and', 'papers', 'therein', '.'], ['the', 'web', 'is', 'a', 'vast', 're-', 'source', 'for', 'many', 'languages', '.'], ['see', 'also', 'banko', 'and', 'brill', '(', '2001', ')', 'for', 'the', 'benefits', 'of', 'large', 'data', 'over', 'sophisticated', 'mathematics', '.'], ['274', 'a.', 'kilgarriff', 'appendix', 'the', 'average', 'value', 'of', 'the', 'error', 'term', 'is', '0.5', '.'], ['we', 'explain', 'this', 'as', 'follows', '.'], ['if', 'we', 'do', 'in', 'fact', 'have', 'a', 'random', 'distribution', ',', 'then', 'by', 'the', 'definition', 'of', 'the', 'χ2', 'distribution', ',', 'the', 'sum', 'of', 'the', 'cells', 'in', 'the', 'contingency', 'table', 'is', '1', ':', 'a⫹b⫹c⫹d⫽1', 'each', 'of', 'these', 'error', 'terms', 'is', 'calculated', 'as', '(', 'o', '⫺', 'e', '⫺', '0.5', ')', '2/e', 'in', 'our', 'situation', ',', 'there', 'are', 'very', 'large', 'datasets', 'and', 'the', 'phenomenon', 'of', 'interest', 'only', 'accounts', 'for', 'a', 'very', 'small', 'proportion', 'of', 'cases', '.'], ['the', 'fre-', 'quency', 'of', 'not', 'word', 'w', 'is', 'very', 'high', '.'], ['thus', 'the', 'expected', 'values', ',', 'e', ',', 'for', 'not', 'word', 'w', 'to', 'be', 'used', 'when', 'calculating', 'c', 'and', 'd', 'for', 'the', 'contingency', 'table', 'are', 'very', 'high', '.'], ['as', 'we', 'divide', 'by', 'very', 'large', 'e', ',', 'c', 'and', 'd', 'are', 'vanishingly', 'small', ',', 'so', 'a⫹b⫹c⫹d⫽1', 'reduces', 'to', 'a⫹b⫽1', 'since', 'we', 'have', 'set', 'the', 'situation', 'up', 'symmetrically', ',', 'a', 'and', 'b', 'are', 'the', 'same', 'size', ',', 'so', 'each', 'will', 'be', ',', 'on', 'average', ',', '0.5', '.'], ['references', 'banko', ',', 'michele', 'and', 'eric', 'brill', '2001', 'scaling', 'to', 'very', 'very', 'large', 'corpora', 'for', 'natural', 'language', 'disambiguation', '.'], ['proceedings', 'of', 'the', '39th', 'annual', 'meeting', 'of', 'the', 'association', 'for', 'computa-', 'tional', 'linguistics', 'and', 'the', '10th', 'conference', 'of', 'the', 'european', 'chapter', 'of', 'the', 'association', 'for', 'computational', 'linguistics', '.'], ['bod', ',', 'rens', '1995', 'enriching', 'linguistics', 'with', 'statistics', ':', 'performance', 'models', 'of', 'natural', 'lan-', 'guage', '.'], ['ph.d.', 'dissertation', ',', 'university', 'of', 'amsterdam', '.'], ['brandstätter', ',', 'e.', '1999', 'confidence', 'intervals', 'as', 'an', 'alternative', 'to', 'significance', 'testing', '.'], ['methods', 'of', 'psychological', 'research', 'outline', '4', '(', '2', ')', ',', '33⫺46', '.'], ['brent', ',', 'michael', 'r.', '1993', 'from', 'grammar', 'to', 'lexicon', ':', 'unsupervised', 'learning', 'of', 'lexical', 'syntax', '.'], ['com-', 'putational', 'linguistics', '19', '(', '2', ')', ',', '243⫺262', '.'], ['briscoe', ',', 'ted', 'and', 'john', 'carroll', '1997', 'automatic', 'extraction', 'of', 'subcategorization', 'from', 'corpora', '.'], ['proceedings', 'of', 'the', 'fifth', 'conference', 'on', 'applied', 'natural', 'language', 'processing', ',', '356⫺363', '.'], ['language', 'is', 'never', ',', 'ever', ',', 'ever', ',', 'random', '275', 'carver', ',', 'r.', 'p.', '1993', 'the', 'case', 'against', 'statistical', 'significance', 'testing', ',', 'revisited', '.'], ['journal', 'of', 'ex-', 'perimental', 'education', '61', ',', '287⫺292', '.'], ['church', ',', 'kenneth', 'and', 'patrick', 'hanks', '1990', 'word', 'association', 'norms', ',', 'mutual', 'information', 'and', 'lexicography', '.'], ['compu-', 'tational', 'linguistics', '16', '(', '1', ')', ',', '22⫺29', '.'], ['dunning', ',', 'ted', '1993', 'accurate', 'methods', 'for', 'the', 'statistics', 'of', 'surprise', 'and', 'coincidence', '.'], ['compu-', 'tational', 'linguistics', '19', '(', '1', ')', ',', '61⫺74', '.'], ['gale', ',', 'william', 'and', 'geoffrey', 'sampson', '1995', 'good-turing', 'frequency', 'estimation', 'without', 'tears', '.'], ['journal', 'of', 'quantitative', 'linguistics', '2', '(', '3', ')', ',', 'good', ',', 'i.', 'j', '.'], ['1953', 'the', 'population', 'frequencies', 'of', 'species', 'and', 'the', 'estimation', 'of', 'population', 'parameters', '.'], ['biometrika', '40', ',', '237⫺264', '.'], ['grefenstette', ',', 'gregory', 'and', 'julien', 'nioche', '2000', 'estimation', 'of', 'english', 'and', 'non-english', 'language', 'use', 'on', 'the', 'www', '.'], ['in', 'proceedings', 'of', 'riao', '(', 'recherche', 'd', '’', 'informations', 'assiste´', 'e', 'par', 'ordinateur', ')', ',', '237⫺246', '.'], ['hofland', ',', 'knud', 'and', 'stig', 'johanson', '(', 'eds', '.', ')'], ['1982', 'word', 'frequencies', 'in', 'british', 'and', 'american', 'english', '.'], ['bergen', ':', 'the', 'norwe-', 'gian', 'computing', 'centre', 'for', 'the', 'humanities', '.'], ['korhonen', ',', 'anna', '2000', 'using', 'semantically', 'motivated', 'estimates', 'to', 'help', 'subcategorization', 'acquisition', '.'], ['proceedings', 'of', 'the', 'joint', 'conference', 'on', 'empirical', 'methods', 'in', 'nlp', 'and', 'very', 'large', 'corpora', ',', '216⫺223', '.'], ['korhonen', ',', 'anna', ',', 'genevieve', 'gorrell', ',', 'and', 'diana', 'mccarthy', '2000', 'statistical', 'filtering', 'and', 'subcategorization', 'frame', 'acquisition', '.'], ['proceedings', 'of', 'the', 'joint', 'conference', 'on', 'empirical', 'methods', 'in', 'nlp', 'and', 'very', 'large', 'corpora', ',', '199⫺206', '.'], ['ldoce', '1995', 'longman', 'dictionary', 'of', 'contemporary', 'english', ',', '3rd', 'edition', '.'], ['ed', '.'], ['della', 'summers', '.'], ['harlow', ':', 'longman', '.'], ['leech', ',', 'geoffrey', 'and', 'roger', 'fallon', '1992', 'computer', 'corpora', '—', 'what', 'do', 'they', 'tell', 'us', 'about', 'culture', '?'], ['icame', 'journal', '16', ',', '29⫺50', '.'], ['manning', ',', 'christopher', 'and', 'hinrich', 'schütze', '1999', 'foundations', 'of', 'statistical', 'natural', 'language', 'processing', '.'], ['cambridge', ',', 'ma', ':', 'mit', 'press', '.'], ['owen', ',', 'frank', 'and', 'ronald', 'jones', '1977', 'statistics', '.'], ['polytech', 'publishers', '.'], ['pedersen', ',', 'ted', '1996', 'fishing', 'for', 'exactness', '.'], ['proceedings', 'of', 'the', 'conference', 'of', 'the', 'south-central', 'sas', 'users', 'group', ',', '188⫺200', '.'], ['rayson', ',', 'paul', 'and', 'roger', 'garside', '2000', 'comparing', 'corpora', 'using', 'frequency', 'profiling', '.'], ['proceedings', 'of', 'the', 'work-', 'shop', 'on', 'comparing', 'corpora', ',', '38th', 'acl', ',', '1⫺6', '.'], ['rayson', ',', 'paul', ',', 'geoffrey', 'leech', ',', 'and', 'mary', 'hodges', '1997', 'social', 'differentiation', 'in', 'the', 'use', 'of', 'english', 'vocabulary', ':', 'some', 'analysis', 'of', 'the', 'conversational', 'component', 'of', 'the', 'british', 'national', 'corpus', '.'], ['interna-', 'tional', 'journal', 'of', 'corpus', 'linguistics', '2', '(', '1', ')', ',', '133⫺152', '.'], ['stubbs', ',', 'michael', '1995', 'collocations', 'and', 'semantic', 'profiles', ':', 'on', 'the', 'cause', 'of', 'the', 'trouble', 'with', 'quantitative', 'studies', '.'], ['functions', 'of', 'language', '2', '(', '1', ')', ',', '23⫺55', '.']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_text[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QYMoK-F_zLRo",
        "outputId": "416bdabe-9a28-4416-a447-00560c7e32e6"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['language',\n",
              " 'is',\n",
              " 'never',\n",
              " ',',\n",
              " 'ever',\n",
              " ',',\n",
              " 'ever',\n",
              " ',',\n",
              " 'random',\n",
              " 'adam',\n",
              " 'kilgarriff',\n",
              " 'abstract',\n",
              " 'language',\n",
              " 'users',\n",
              " 'never',\n",
              " 'choose',\n",
              " 'words',\n",
              " 'randomly',\n",
              " ',',\n",
              " 'and',\n",
              " 'language',\n",
              " 'is',\n",
              " 'essentially',\n",
              " 'non-random',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.lm import MLE\n",
        "\n",
        "n = 3\n",
        "train_data, padded_sents = padded_everygram_pipeline(n, tokenized_text)\n",
        "\n",
        "model = MLE(n) # Lets train a 3-grams model, previously we set n=3"
      ],
      "metadata": {
        "id": "uXrd-mrPzMDk"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(train_data, padded_sents)\n",
        "print(model.vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ckDAk3uazM2D",
        "outputId": "3a3735e3-625b-4489-918c-d37326e295df"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<Vocabulary with cutoff=1 unk_label='<UNK>' and 1391 items>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(model.vocab.lookup('language is never random lah .'.split()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zr4zgz5ezNaY",
        "outputId": "4fe7015f-ecf4-4dc3-e8ec-2e2f24097c11"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('language', 'is', 'never', 'random', '<UNK>', '.')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.counts['language'] # i.e. Count('language')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OTwkPIUbzOHl",
        "outputId": "30625b53-1773-4bda-9702-54967c220235"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "25"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.counts[['language']]['is'] # i.e. Count('is'|'language')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jMIec9q5zO06",
        "outputId": "9fb5467b-3a0a-4cb4-f796-110fdcbd64df"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "11"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.counts[['language', 'is']]['never'] # i.e. Count('never'|'language is')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zp9XddNezPj9",
        "outputId": "182b653e-c06c-4286-c9ba-41afbf367a9f"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.score('language') # P('language')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ohL3NsNRzQBx",
        "outputId": "fdf8c93c-87fb-41b8-ecd6-8983a63400e7"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.003691671588895452"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.score('is', 'language'.split())  # P('is'|'language')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "de6gm0_WzQn_",
        "outputId": "1136ee4a-d8fd-413d-e7a1-59129d0635d7"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.44"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.score('never', 'language is'.split())  # P('never'|'language is')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O9ySI5vLzRH8",
        "outputId": "8007b6cc-0d14-4817-a8d4-4f6c3325ab63"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6363636363636364"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(model.generate(20, random_seed=7))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RWpjXpBtzRq2",
        "outputId": "436c3dbb-a540-4e68-82fd-65b3273c735b"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['and', 'carroll', 'used', 'hypothesis', 'testing', 'has', 'been', 'used', ',', 'and', 'a', 'half', '.', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
        "\n",
        "detokenize = TreebankWordDetokenizer().detokenize\n",
        "\n",
        "def generate_sent(model, num_words, random_seed=42):\n",
        "    \"\"\"\n",
        "    :param model: An ngram language model from `nltk.lm.model`.\n",
        "    :param num_words: Max no. of words to generate.\n",
        "    :param random_seed: Seed value for random.\n",
        "    \"\"\"\n",
        "    content = []\n",
        "    for token in model.generate(num_words, random_seed=random_seed):\n",
        "        if token == '<s>':\n",
        "            continue\n",
        "        if token == '</s>':\n",
        "            break\n",
        "        content.append(token)\n",
        "    return detokenize(content)\n",
        "\n",
        "generate_sent(model, 20, random_seed=7)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "8eyYs6qkzSKa",
        "outputId": "e86772a8-4663-41fd-969a-dde9dfc59f78"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'and carroll used hypothesis testing has been used, and a half.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets download miguelaenlle/massive-stock-news-analysis-db-for-nlpbacktests"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2e-cpW6DzSsh",
        "outputId": "726f7919-7a0a-4766-896c-7d7f371fe74e"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/miguelaenlle/massive-stock-news-analysis-db-for-nlpbacktests\n",
            "License(s): CC0-1.0\n",
            "Downloading massive-stock-news-analysis-db-for-nlpbacktests.zip to /content\n",
            " 99% 208M/210M [00:12<00:00, 19.8MB/s]\n",
            "100% 210M/210M [00:12<00:00, 18.0MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip massive-stock-news-analysis-db-for-nlpbacktests.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "elan0JYGzTh4",
        "outputId": "b0192af8-d62c-456d-ae58-c359dc5299bb"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  massive-stock-news-analysis-db-for-nlpbacktests.zip\n",
            "  inflating: analyst_ratings_processed.csv  \n",
            "  inflating: raw_analyst_ratings.csv  \n",
            "  inflating: raw_partner_headlines.csv  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('raw_partner_headlines.csv')\n",
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "tYi9HUVkzUN2",
        "outputId": "070ecf91-68ab-438f-b96c-27d208b2f09f"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Unnamed: 0                                           headline  \\\n",
              "0           2  Agilent Technologies Announces Pricing of $5……...   \n",
              "1           3  Agilent (A) Gears Up for Q2 Earnings: What's i...   \n",
              "2           4  J.P. Morgan Asset Management Announces Liquida...   \n",
              "3           5  Pershing Square Capital Management, L.P. Buys ...   \n",
              "4           6  Agilent Awards Trilogy Sciences with a Golden ...   \n",
              "\n",
              "                                                 url  publisher  \\\n",
              "0  http://www.gurufocus.com/news/1153187/agilent-...  GuruFocus   \n",
              "1  http://www.zacks.com/stock/news/931205/agilent...      Zacks   \n",
              "2  http://www.gurufocus.com/news/1138923/jp-morga...  GuruFocus   \n",
              "3  http://www.gurufocus.com/news/1138704/pershing...  GuruFocus   \n",
              "4  http://www.gurufocus.com/news/1134012/agilent-...  GuruFocus   \n",
              "\n",
              "                  date stock  \n",
              "0  2020-06-01 00:00:00     A  \n",
              "1  2020-05-18 00:00:00     A  \n",
              "2  2020-05-15 00:00:00     A  \n",
              "3  2020-05-15 00:00:00     A  \n",
              "4  2020-05-12 00:00:00     A  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-518c977f-f8dd-4dcc-8120-66ef4d104004\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>headline</th>\n",
              "      <th>url</th>\n",
              "      <th>publisher</th>\n",
              "      <th>date</th>\n",
              "      <th>stock</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2</td>\n",
              "      <td>Agilent Technologies Announces Pricing of $5……...</td>\n",
              "      <td>http://www.gurufocus.com/news/1153187/agilent-...</td>\n",
              "      <td>GuruFocus</td>\n",
              "      <td>2020-06-01 00:00:00</td>\n",
              "      <td>A</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3</td>\n",
              "      <td>Agilent (A) Gears Up for Q2 Earnings: What's i...</td>\n",
              "      <td>http://www.zacks.com/stock/news/931205/agilent...</td>\n",
              "      <td>Zacks</td>\n",
              "      <td>2020-05-18 00:00:00</td>\n",
              "      <td>A</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4</td>\n",
              "      <td>J.P. Morgan Asset Management Announces Liquida...</td>\n",
              "      <td>http://www.gurufocus.com/news/1138923/jp-morga...</td>\n",
              "      <td>GuruFocus</td>\n",
              "      <td>2020-05-15 00:00:00</td>\n",
              "      <td>A</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>5</td>\n",
              "      <td>Pershing Square Capital Management, L.P. Buys ...</td>\n",
              "      <td>http://www.gurufocus.com/news/1138704/pershing...</td>\n",
              "      <td>GuruFocus</td>\n",
              "      <td>2020-05-15 00:00:00</td>\n",
              "      <td>A</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>6</td>\n",
              "      <td>Agilent Awards Trilogy Sciences with a Golden ...</td>\n",
              "      <td>http://www.gurufocus.com/news/1134012/agilent-...</td>\n",
              "      <td>GuruFocus</td>\n",
              "      <td>2020-05-12 00:00:00</td>\n",
              "      <td>A</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-518c977f-f8dd-4dcc-8120-66ef4d104004')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-518c977f-f8dd-4dcc-8120-66ef4d104004 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-518c977f-f8dd-4dcc-8120-66ef4d104004');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-c1c3d377-93dd-4e61-82f1-f220061cb378\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-c1c3d377-93dd-4e61-82f1-f220061cb378')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-c1c3d377-93dd-4e61-82f1-f220061cb378 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df"
            }
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "news = []\n",
        "for i, j in df.iterrows():\n",
        "    news.append(j['headline'])\n",
        "\n",
        "print(len(news))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MFLp-TgtzU0a",
        "outputId": "dbce3e72-c19b-4c68-914d-c808081424ce"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1845559\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "news = news[:109233]"
      ],
      "metadata": {
        "id": "oTQ63nfczVa7"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('finance_news.txt', 'w') as f:\n",
        "    f.write('\\n'.join(news))"
      ],
      "metadata": {
        "id": "h6PSfg9XzWKZ"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pickle\n",
        "import torch\n",
        "\n",
        "SPECIAL_WORDS = {'PADDING': '<PAD>'}\n",
        "\n",
        "def load_data(path):\n",
        "    \"\"\"\n",
        "    Load Dataset from File\n",
        "    \"\"\"\n",
        "    input_file = os.path.join(path)\n",
        "    with open(input_file, \"r\") as f:\n",
        "        data = f.read()\n",
        "\n",
        "    return data\n",
        "\n",
        "data_dir = 'finance_news.txt'\n",
        "text = load_data(data_dir)"
      ],
      "metadata": {
        "id": "HWwRS2TKzWyE"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "view_line_range = (0, 10)\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "print('Dataset Stats')\n",
        "print('Roughly the number of unique words: {}'.format(len({word: None for word in text.split()})))\n",
        "\n",
        "lines = text.split('\\n')\n",
        "print('Number of lines: {}'.format(len(lines)))\n",
        "word_count_line = [len(line.split()) for line in lines]\n",
        "print('Average number of words in each line: {}'.format(np.average(word_count_line)))\n",
        "\n",
        "print()\n",
        "print('The lines {} to {}:'.format(*view_line_range))\n",
        "print('\\n'.join(text.split('\\n')[view_line_range[0]:view_line_range[1]]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w1KDXRU3zXqm",
        "outputId": "9e1b38d5-bd02-440d-c3c1-7b1fae331343"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset Stats\n",
            "Roughly the number of unique words: 58299\n",
            "Number of lines: 109233\n",
            "Average number of words in each line: 9.44242124632666\n",
            "\n",
            "The lines 0 to 10:\n",
            "Agilent Technologies Announces Pricing of $5…… Million of Senior Notes\n",
            "Agilent (A) Gears Up for Q2 Earnings: What's in the Cards?\n",
            "J.P. Morgan Asset Management Announces Liquidation of Six Exchange-Traded Funds\n",
            "Pershing Square Capital Management, L.P. Buys Agilent Technologies Inc, The Howard Hughes Corp, ...\n",
            "Agilent Awards Trilogy Sciences with a Golden Ticket at LabCentral\n",
            "Agilent Technologies Inc (A) CEO and President Michael R. Mcmullen Sold $–.4 million of Shares\n",
            "' Stocks Growing Their Earnings Fast\n",
            "Cypress Asset Management Inc Buys Verizon Communications Inc, United Parcel Service Inc, ...\n",
            "Hendley & Co Inc Buys American Electric Power Co Inc, Agilent Technologies Inc, Paychex ...\n",
            "Teacher Retirement System Of Texas Buys Hologic Inc, Vanguard Total Stock Market, Agilent ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_and_save_data(dataset_path, token_lookup, create_lookup_tables):\n",
        "    \"\"\"\n",
        "    Preprocess Text Data\n",
        "    \"\"\"\n",
        "    text = load_data(dataset_path)\n",
        "\n",
        "    # Ignore notice, since we don't use it for analysing the data\n",
        "    text = text[81:]\n",
        "\n",
        "    token_dict = token_lookup()\n",
        "    for key, token in token_dict.items():\n",
        "        text = text.replace(key, ' {} '.format(token))\n",
        "\n",
        "    text = text.lower()\n",
        "    text = text.split()\n",
        "\n",
        "    vocab_to_int, int_to_vocab = create_lookup_tables(text + list(SPECIAL_WORDS.values()))\n",
        "    int_text = [vocab_to_int[word] for word in text]\n",
        "    pickle.dump((int_text, vocab_to_int, int_to_vocab, token_dict), open('preprocess.p', 'wb'))\n",
        "\n",
        "\n",
        "def load_preprocess():\n",
        "    \"\"\"\n",
        "    Load the Preprocessed Training data and return them in batches of <batch_size> or less\n",
        "    \"\"\"\n",
        "    return pickle.load(open('preprocess.p', mode='rb'))\n",
        "\n",
        "\n",
        "def save_model(filename, decoder):\n",
        "    save_filename = os.path.splitext(os.path.basename(filename))[0] + '.pt'\n",
        "    torch.save(decoder, save_filename)\n",
        "\n",
        "\n",
        "def load_model(filename):\n",
        "    save_filename = os.path.splitext(os.path.basename(filename))[0] + '.pt'\n",
        "    return torch.load(save_filename)"
      ],
      "metadata": {
        "id": "24ZZOm9bzYVO"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "def create_lookup_tables(text):\n",
        "    \"\"\"\n",
        "    Create lookup tables for vocabulary\n",
        "    :param text: The text of tv scripts split into words\n",
        "    :return: A tuple of dicts (vocab_to_int, int_to_vocab)\n",
        "    \"\"\"\n",
        "    # TODO: Implement Function\n",
        "    word_count = Counter(text)\n",
        "    sorted_vocab = sorted(word_count, key = word_count.get, reverse=True)\n",
        "    int_to_vocab = {ii:word for ii, word in enumerate(sorted_vocab)}\n",
        "    vocab_to_int = {word:ii for ii, word in int_to_vocab.items()}\n",
        "\n",
        "    # return tuple\n",
        "    return (vocab_to_int, int_to_vocab)\n",
        "\n",
        "def token_lookup():\n",
        "    \"\"\"\n",
        "    Generate a dict to turn punctuation into a token.\n",
        "    :return: Tokenized dictionary where the key is the punctuation and the value is the token\n",
        "    \"\"\"\n",
        "    # TODO: Implement Function\n",
        "    token = dict()\n",
        "    token['.'] = '<PERIOD>'\n",
        "    token[','] = '<COMMA>'\n",
        "    token['\"'] = 'QUOTATION_MARK'\n",
        "    token[';'] = 'SEMICOLON'\n",
        "    token['!'] = 'EXCLAIMATION_MARK'\n",
        "    token['?'] = 'QUESTION_MARK'\n",
        "    token['('] = 'LEFT_PAREN'\n",
        "    token[')'] = 'RIGHT_PAREN'\n",
        "    token['-'] = 'QUESTION_MARK'\n",
        "    token['\\n'] = 'NEW_LINE'\n",
        "    return token\n",
        "\n",
        "preprocess_and_save_data(data_dir, token_lookup, create_lookup_tables)\n",
        "\n",
        "int_text, vocab_to_int, int_to_vocab, token_dict = load_preprocess()\n",
        "\n",
        "train_on_gpu = torch.cuda.is_available()"
      ],
      "metadata": {
        "id": "l0c8zTKozZCP"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "def batch_data(words, sequence_length, batch_size):\n",
        "    \"\"\"\n",
        "    Batch the neural network data using DataLoader\n",
        "    :param words: The word ids of the TV scripts\n",
        "    :param sequence_length: The sequence length of each batch\n",
        "    :param batch_size: The size of each batch; the number of sequences in a batch\n",
        "    :return: DataLoader with batched data\n",
        "    \"\"\"\n",
        "    # TODO: Implement function\n",
        "    n_batches = len(words)//batch_size\n",
        "    x, y = [], []\n",
        "    words = words[:n_batches*batch_size]\n",
        "\n",
        "    for ii in range(0, len(words)-sequence_length):\n",
        "        i_end = ii+sequence_length\n",
        "        batch_x = words[ii:ii+sequence_length]\n",
        "        x.append(batch_x)\n",
        "        batch_y = words[i_end]\n",
        "        y.append(batch_y)\n",
        "\n",
        "    data = TensorDataset(torch.from_numpy(np.asarray(x)), torch.from_numpy(np.asarray(y)))\n",
        "    data_loader = DataLoader(data, shuffle=True, batch_size=batch_size)\n",
        "\n",
        "\n",
        "    # return a dataloader\n",
        "    return data_loader"
      ],
      "metadata": {
        "id": "Q5emFlfpzZq8"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test dataloader\n",
        "\n",
        "test_text = range(50)\n",
        "t_loader = batch_data(test_text, sequence_length=5, batch_size=10)\n",
        "\n",
        "data_iter = iter(t_loader)\n",
        "sample_x, sample_y = next(data_iter)\n",
        "\n",
        "print(sample_x.shape)\n",
        "print(sample_x)\n",
        "print()\n",
        "print(sample_y.shape)\n",
        "print(sample_y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v5bfjO5szaQp",
        "outputId": "b7c553a1-8c39-4597-d15c-07568ef136ae"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([10, 5])\n",
            "tensor([[10, 11, 12, 13, 14],\n",
            "        [31, 32, 33, 34, 35],\n",
            "        [15, 16, 17, 18, 19],\n",
            "        [ 0,  1,  2,  3,  4],\n",
            "        [24, 25, 26, 27, 28],\n",
            "        [22, 23, 24, 25, 26],\n",
            "        [20, 21, 22, 23, 24],\n",
            "        [11, 12, 13, 14, 15],\n",
            "        [ 6,  7,  8,  9, 10],\n",
            "        [ 3,  4,  5,  6,  7]])\n",
            "\n",
            "torch.Size([10])\n",
            "tensor([15, 36, 20,  5, 29, 27, 25, 16, 11,  8])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class RNN(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, dropout=0.5):\n",
        "        \"\"\"\n",
        "        Initialize the PyTorch RNN Module\n",
        "        :param vocab_size: The number of input dimensions of the neural network (the size of the vocabulary)\n",
        "        :param output_size: The number of output dimensions of the neural network\n",
        "        :param embedding_dim: The size of embeddings, should you choose to use them\n",
        "        :param hidden_dim: The size of the hidden layer outputs\n",
        "        :param dropout: dropout to add in between RNN layers\n",
        "        \"\"\"\n",
        "        super(RNN, self).__init__()\n",
        "        # TODO: Implement function\n",
        "\n",
        "        # define embedding layer\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "        # define rnn layer\n",
        "        self.rnn = nn.LSTM(embedding_dim, hidden_dim, n_layers, dropout=dropout, batch_first=True)\n",
        "\n",
        "        # set class variables\n",
        "        self.vocab_size = vocab_size\n",
        "        self.output_size = output_size\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.n_layers = n_layers\n",
        "\n",
        "        # define model layers\n",
        "        self.fc = nn.Linear(hidden_dim, output_size)\n",
        "\n",
        "    def forward(self, x, hidden):\n",
        "        \"\"\"\n",
        "        Forward propagation of the neural network\n",
        "        :param nn_input: The input to the neural network\n",
        "        :param hidden: The hidden state\n",
        "        :return: Two Tensors, the output of the neural network and the latest hidden state\n",
        "        \"\"\"\n",
        "        # TODO: Implement function\n",
        "        batch_size = x.size(0)\n",
        "        x=x.long()\n",
        "\n",
        "        # embedding and rnn_out\n",
        "        embeds = self.embedding(x)\n",
        "        rnn_out, hidden = self.rnn(embeds, hidden)\n",
        "\n",
        "        # stack up lstm layers\n",
        "        rnn_out = rnn_out.contiguous().view(-1, self.hidden_dim)\n",
        "\n",
        "        # dropout, fc layer and final sigmoid layer\n",
        "        out = self.fc(rnn_out)\n",
        "\n",
        "        # reshaping out layer to batch_size * seq_length * output_size\n",
        "        out = out.view(batch_size, -1, self.output_size)\n",
        "\n",
        "        # return last batch\n",
        "        out = out[:, -1]\n",
        "\n",
        "        # return one batch of output word scores and the hidden state\n",
        "        return out, hidden\n",
        "\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        '''\n",
        "        Initialize the hidden state of an LSTM/GRU\n",
        "        :param batch_size: The batch_size of the hidden state\n",
        "        :return: hidden state of dims (n_layers, batch_size, hidden_dim)\n",
        "        '''\n",
        "        # create 2 new zero tensors of size n_layers * batch_size * hidden_dim\n",
        "        weights = next(self.parameters()).data\n",
        "        if(train_on_gpu):\n",
        "            hidden = (weights.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda(),\n",
        "                     weights.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda())\n",
        "        else:\n",
        "            hidden = (weights.new(self.n_layers, batch_size, self.hidden_dim).zero_(),\n",
        "                     weights.new(self.n_layers, batch_size, self.hidden_dim).zero_())\n",
        "\n",
        "        # initialize hidden state with zero weights, and move to GPU if available\n",
        "\n",
        "        return hidden"
      ],
      "metadata": {
        "id": "iYdzxq1Bzazh"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def forward_back_prop(rnn, optimizer, criterion, inp, target, hidden):\n",
        "    \"\"\"\n",
        "    Forward and backward propagation on the neural network\n",
        "    :param decoder: The PyTorch Module that holds the neural network\n",
        "    :param decoder_optimizer: The PyTorch optimizer for the neural network\n",
        "    :param criterion: The PyTorch loss function\n",
        "    :param inp: A batch of input to the neural network\n",
        "    :param target: The target output for the batch of input\n",
        "    :return: The loss and the latest hidden state Tensor\n",
        "    \"\"\"\n",
        "\n",
        "    # TODO: Implement Function\n",
        "\n",
        "    # move data to GPU, if available\n",
        "    if(train_on_gpu):\n",
        "        rnn.cuda()\n",
        "\n",
        "    # creating variables for hidden state to prevent back-propagation\n",
        "    # of historical states\n",
        "    h = tuple([each.data for each in hidden])\n",
        "\n",
        "    rnn.zero_grad()\n",
        "    # move inputs, targets to GPU\n",
        "    inputs, targets = inp.cuda(), target.cuda()\n",
        "\n",
        "    output, h = rnn(inputs, h)\n",
        "\n",
        "    loss = criterion(output, targets)\n",
        "\n",
        "    # perform backpropagation and optimization\n",
        "    loss.backward()\n",
        "    nn.utils.clip_grad_norm_(rnn.parameters(), 5)\n",
        "    optimizer.step()\n",
        "\n",
        "    # return the loss over a batch and the hidden state produced by our model\n",
        "    return loss.item(), h"
      ],
      "metadata": {
        "id": "NKDsXGcUzbXL"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_rnn(rnn, batch_size, optimizer, criterion, n_epochs, show_every_n_batches=100):\n",
        "    batch_losses = []\n",
        "\n",
        "    rnn.train()\n",
        "\n",
        "    print(\"Training for %d epoch(s)...\" % n_epochs)\n",
        "    for epoch_i in range(1, n_epochs + 1):\n",
        "\n",
        "        # initialize hidden state\n",
        "        hidden = rnn.init_hidden(batch_size)\n",
        "\n",
        "        for batch_i, (inputs, labels) in enumerate(train_loader, 1):\n",
        "\n",
        "            # make sure you iterate over completely full batches, only\n",
        "            n_batches = len(train_loader.dataset)//batch_size\n",
        "            if(batch_i > n_batches):\n",
        "                break\n",
        "\n",
        "            # forward, back prop\n",
        "            loss, hidden = forward_back_prop(rnn, optimizer, criterion, inputs, labels, hidden)\n",
        "            # record loss\n",
        "            batch_losses.append(loss)\n",
        "\n",
        "            # printing loss stats\n",
        "            if batch_i % show_every_n_batches == 0:\n",
        "                print('Epoch: {:>4}/{:<4}  Loss: {}\\n'.format(\n",
        "                    epoch_i, n_epochs, np.average(batch_losses)))\n",
        "                batch_losses = []\n",
        "\n",
        "    # returns a trained rnn\n",
        "    return rnn"
      ],
      "metadata": {
        "id": "Pnz2e4Ypzb9x"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data params\n",
        "# Sequence Length\n",
        "sequence_length = 10  # of words in a sequence\n",
        "# Batch Size\n",
        "batch_size = 128\n",
        "\n",
        "# data loader - do not change\n",
        "train_loader = batch_data(int_text, sequence_length, batch_size)\n",
        "\n",
        "# Training parameters\n",
        "# Number of Epochs\n",
        "num_epochs = 10\n",
        "# Learning Rate\n",
        "learning_rate = 0.001\n",
        "\n",
        "# Model parameters\n",
        "# Vocab size\n",
        "vocab_size = len(vocab_to_int)\n",
        "# Output size\n",
        "output_size = vocab_size\n",
        "# Embedding Dimension\n",
        "embedding_dim = 200\n",
        "# Hidden Dimension\n",
        "hidden_dim = 250\n",
        "# Number of RNN Layers\n",
        "n_layers = 2\n",
        "\n",
        "# Show stats for every n number of batches\n",
        "show_every_n_batches = 500"
      ],
      "metadata": {
        "id": "2y5fRtvWzcik"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create model and move to gpu if available\n",
        "rnn = RNN(vocab_size, output_size, embedding_dim, hidden_dim, n_layers, dropout=0.5)\n",
        "if train_on_gpu:\n",
        "    rnn.cuda()\n",
        "\n",
        "# defining loss and optimization functions for training\n",
        "optimizer = torch.optim.Adam(rnn.parameters(), lr=learning_rate)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# training the model\n",
        "trained_rnn = train_rnn(rnn, batch_size, optimizer, criterion, num_epochs, show_every_n_batches)\n",
        "\n",
        "# saving the trained model\n",
        "save_model('./save/trained_rnn', trained_rnn)\n",
        "print('Model Trained and Saved')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GXAbyvwtzdJo",
        "outputId": "c65e8c4d-f70a-4310-941a-2d64122c7154"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training for 10 epoch(s)...\n",
            "Epoch:    1/10    Loss: 7.001492801666259\n",
            "\n",
            "Epoch:    1/10    Loss: 6.198969908714295\n",
            "\n",
            "Epoch:    1/10    Loss: 5.782324034690857\n",
            "\n",
            "Epoch:    1/10    Loss: 5.517343311309815\n",
            "\n",
            "Epoch:    1/10    Loss: 5.342130349159241\n",
            "\n",
            "Epoch:    1/10    Loss: 5.196880195140839\n",
            "\n",
            "Epoch:    1/10    Loss: 5.082298403739929\n",
            "\n",
            "Epoch:    1/10    Loss: 4.961610969543457\n",
            "\n",
            "Epoch:    1/10    Loss: 4.924522009849548\n",
            "\n",
            "Epoch:    1/10    Loss: 4.829055475711822\n",
            "\n",
            "Epoch:    1/10    Loss: 4.747038439750671\n",
            "\n",
            "Epoch:    1/10    Loss: 4.665092332839966\n",
            "\n",
            "Epoch:    1/10    Loss: 4.6087429904937744\n",
            "\n",
            "Epoch:    1/10    Loss: 4.5646375794410705\n",
            "\n",
            "Epoch:    1/10    Loss: 4.518545735836029\n",
            "\n",
            "Epoch:    1/10    Loss: 4.477399680137634\n",
            "\n",
            "Epoch:    1/10    Loss: 4.430985699176788\n",
            "\n",
            "Epoch:    1/10    Loss: 4.4114632987976075\n",
            "\n",
            "Epoch:    1/10    Loss: 4.390264133453369\n",
            "\n",
            "Epoch:    1/10    Loss: 4.36915919303894\n",
            "\n",
            "Epoch:    2/10    Loss: 4.19429008045124\n",
            "\n",
            "Epoch:    2/10    Loss: 4.102066344738007\n",
            "\n",
            "Epoch:    2/10    Loss: 4.066066747665405\n",
            "\n",
            "Epoch:    2/10    Loss: 4.060636493206024\n",
            "\n",
            "Epoch:    2/10    Loss: 4.035756554126739\n",
            "\n",
            "Epoch:    2/10    Loss: 4.024183757781983\n",
            "\n",
            "Epoch:    2/10    Loss: 4.003660956859589\n",
            "\n",
            "Epoch:    2/10    Loss: 4.0083083491325375\n",
            "\n",
            "Epoch:    2/10    Loss: 4.018723602294922\n",
            "\n",
            "Epoch:    2/10    Loss: 3.999146508216858\n",
            "\n",
            "Epoch:    2/10    Loss: 3.9801129837036133\n",
            "\n",
            "Epoch:    2/10    Loss: 3.9726451516151426\n",
            "\n",
            "Epoch:    2/10    Loss: 3.973916963100433\n",
            "\n",
            "Epoch:    2/10    Loss: 3.9433376998901366\n",
            "\n",
            "Epoch:    2/10    Loss: 3.97390492105484\n",
            "\n",
            "Epoch:    2/10    Loss: 3.9546197714805604\n",
            "\n",
            "Epoch:    2/10    Loss: 3.9419301600456236\n",
            "\n",
            "Epoch:    2/10    Loss: 3.9075502133369446\n",
            "\n",
            "Epoch:    2/10    Loss: 3.8981599864959717\n",
            "\n",
            "Epoch:    2/10    Loss: 3.9284100170135496\n",
            "\n",
            "Epoch:    3/10    Loss: 3.766213386835405\n",
            "\n",
            "Epoch:    3/10    Loss: 3.6892167348861693\n",
            "\n",
            "Epoch:    3/10    Loss: 3.668491768360138\n",
            "\n",
            "Epoch:    3/10    Loss: 3.6822539649009705\n",
            "\n",
            "Epoch:    3/10    Loss: 3.6810650053024294\n",
            "\n",
            "Epoch:    3/10    Loss: 3.7007436361312864\n",
            "\n",
            "Epoch:    3/10    Loss: 3.6950982117652895\n",
            "\n",
            "Epoch:    3/10    Loss: 3.6897907662391662\n",
            "\n",
            "Epoch:    3/10    Loss: 3.7046893463134767\n",
            "\n",
            "Epoch:    3/10    Loss: 3.673771071910858\n",
            "\n",
            "Epoch:    3/10    Loss: 3.6868925857543946\n",
            "\n",
            "Epoch:    3/10    Loss: 3.6719315786361695\n",
            "\n",
            "Epoch:    3/10    Loss: 3.689346580505371\n",
            "\n",
            "Epoch:    3/10    Loss: 3.699502715587616\n",
            "\n",
            "Epoch:    3/10    Loss: 3.682773003578186\n",
            "\n",
            "Epoch:    3/10    Loss: 3.693020718097687\n",
            "\n",
            "Epoch:    3/10    Loss: 3.6846333885192872\n",
            "\n",
            "Epoch:    3/10    Loss: 3.6898608078956605\n",
            "\n",
            "Epoch:    3/10    Loss: 3.7044062814712526\n",
            "\n",
            "Epoch:    3/10    Loss: 3.6978643274307252\n",
            "\n",
            "Epoch:    4/10    Loss: 3.552692603097971\n",
            "\n",
            "Epoch:    4/10    Loss: 3.4712283096313477\n",
            "\n",
            "Epoch:    4/10    Loss: 3.4870718936920166\n",
            "\n",
            "Epoch:    4/10    Loss: 3.482079204559326\n",
            "\n",
            "Epoch:    4/10    Loss: 3.4795208868980407\n",
            "\n",
            "Epoch:    4/10    Loss: 3.4998470039367677\n",
            "\n",
            "Epoch:    4/10    Loss: 3.4840475926399233\n",
            "\n",
            "Epoch:    4/10    Loss: 3.501112688064575\n",
            "\n",
            "Epoch:    4/10    Loss: 3.5039537725448606\n",
            "\n",
            "Epoch:    4/10    Loss: 3.5103283696174623\n",
            "\n",
            "Epoch:    4/10    Loss: 3.503017372131348\n",
            "\n",
            "Epoch:    4/10    Loss: 3.5201694788932802\n",
            "\n",
            "Epoch:    4/10    Loss: 3.5176790308952333\n",
            "\n",
            "Epoch:    4/10    Loss: 3.543485216140747\n",
            "\n",
            "Epoch:    4/10    Loss: 3.546807415008545\n",
            "\n",
            "Epoch:    4/10    Loss: 3.5445794520378113\n",
            "\n",
            "Epoch:    4/10    Loss: 3.5453635396957397\n",
            "\n",
            "Epoch:    4/10    Loss: 3.5296778717041017\n",
            "\n",
            "Epoch:    4/10    Loss: 3.557270013809204\n",
            "\n",
            "Epoch:    4/10    Loss: 3.5692304496765135\n",
            "\n",
            "Epoch:    5/10    Loss: 3.394788018650882\n",
            "\n",
            "Epoch:    5/10    Loss: 3.3493208093643188\n",
            "\n",
            "Epoch:    5/10    Loss: 3.327899149417877\n",
            "\n",
            "Epoch:    5/10    Loss: 3.3705668716430663\n",
            "\n",
            "Epoch:    5/10    Loss: 3.3783716564178468\n",
            "\n",
            "Epoch:    5/10    Loss: 3.3677116956710815\n",
            "\n",
            "Epoch:    5/10    Loss: 3.399818745136261\n",
            "\n",
            "Epoch:    5/10    Loss: 3.375498830795288\n",
            "\n",
            "Epoch:    5/10    Loss: 3.4091563215255736\n",
            "\n",
            "Epoch:    5/10    Loss: 3.3385024180412293\n",
            "\n",
            "Epoch:    5/10    Loss: 3.3881943225860596\n",
            "\n",
            "Epoch:    5/10    Loss: 3.407322458744049\n",
            "\n",
            "Epoch:    5/10    Loss: 3.393529588699341\n",
            "\n",
            "Epoch:    5/10    Loss: 3.4130230507850645\n",
            "\n",
            "Epoch:    5/10    Loss: 3.413640005111694\n",
            "\n",
            "Epoch:    5/10    Loss: 3.4171880197525026\n",
            "\n",
            "Epoch:    5/10    Loss: 3.4289389271736144\n",
            "\n",
            "Epoch:    5/10    Loss: 3.4344843492507935\n",
            "\n",
            "Epoch:    5/10    Loss: 3.438580561161041\n",
            "\n",
            "Epoch:    5/10    Loss: 3.432901093006134\n",
            "\n",
            "Epoch:    6/10    Loss: 3.308874706502801\n",
            "\n",
            "Epoch:    6/10    Loss: 3.2237092561721803\n",
            "\n",
            "Epoch:    6/10    Loss: 3.249630163192749\n",
            "\n",
            "Epoch:    6/10    Loss: 3.2434720439910887\n",
            "\n",
            "Epoch:    6/10    Loss: 3.272429856300354\n",
            "\n",
            "Epoch:    6/10    Loss: 3.274553696632385\n",
            "\n",
            "Epoch:    6/10    Loss: 3.2749985928535463\n",
            "\n",
            "Epoch:    6/10    Loss: 3.2582083859443665\n",
            "\n",
            "Epoch:    6/10    Loss: 3.2975568542480467\n",
            "\n",
            "Epoch:    6/10    Loss: 3.3000725450515747\n",
            "\n",
            "Epoch:    6/10    Loss: 3.3288259377479554\n",
            "\n",
            "Epoch:    6/10    Loss: 3.309932170391083\n",
            "\n",
            "Epoch:    6/10    Loss: 3.3120308799743654\n",
            "\n",
            "Epoch:    6/10    Loss: 3.330663513660431\n",
            "\n",
            "Epoch:    6/10    Loss: 3.3292673377990725\n",
            "\n",
            "Epoch:    6/10    Loss: 3.340408859729767\n",
            "\n",
            "Epoch:    6/10    Loss: 3.3397026629447937\n",
            "\n",
            "Epoch:    6/10    Loss: 3.3467573957443237\n",
            "\n",
            "Epoch:    6/10    Loss: 3.3535412969589236\n",
            "\n",
            "Epoch:    6/10    Loss: 3.398132154464722\n",
            "\n",
            "Epoch:    7/10    Loss: 3.229475555915494\n",
            "\n",
            "Epoch:    7/10    Loss: 3.1452893209457398\n",
            "\n",
            "Epoch:    7/10    Loss: 3.181766236305237\n",
            "\n",
            "Epoch:    7/10    Loss: 3.1816138586997984\n",
            "\n",
            "Epoch:    7/10    Loss: 3.1726620874404907\n",
            "\n",
            "Epoch:    7/10    Loss: 3.19467290353775\n",
            "\n",
            "Epoch:    7/10    Loss: 3.179736294746399\n",
            "\n",
            "Epoch:    7/10    Loss: 3.211739363193512\n",
            "\n",
            "Epoch:    7/10    Loss: 3.213245973587036\n",
            "\n",
            "Epoch:    7/10    Loss: 3.2449643478393555\n",
            "\n",
            "Epoch:    7/10    Loss: 3.232411657810211\n",
            "\n",
            "Epoch:    7/10    Loss: 3.255715602874756\n",
            "\n",
            "Epoch:    7/10    Loss: 3.245994426727295\n",
            "\n",
            "Epoch:    7/10    Loss: 3.2850151810646055\n",
            "\n",
            "Epoch:    7/10    Loss: 3.248828628063202\n",
            "\n",
            "Epoch:    7/10    Loss: 3.2492163200378417\n",
            "\n",
            "Epoch:    7/10    Loss: 3.2980582675933836\n",
            "\n",
            "Epoch:    7/10    Loss: 3.323276687145233\n",
            "\n",
            "Epoch:    7/10    Loss: 3.307746425151825\n",
            "\n",
            "Epoch:    7/10    Loss: 3.2840438408851624\n",
            "\n",
            "Epoch:    8/10    Loss: 3.1668405886385376\n",
            "\n",
            "Epoch:    8/10    Loss: 3.089994870185852\n",
            "\n",
            "Epoch:    8/10    Loss: 3.106421217918396\n",
            "\n",
            "Epoch:    8/10    Loss: 3.111894748210907\n",
            "\n",
            "Epoch:    8/10    Loss: 3.1387496552467344\n",
            "\n",
            "Epoch:    8/10    Loss: 3.1278502440452574\n",
            "\n",
            "Epoch:    8/10    Loss: 3.1578492650985717\n",
            "\n",
            "Epoch:    8/10    Loss: 3.1771907362937926\n",
            "\n",
            "Epoch:    8/10    Loss: 3.156777626991272\n",
            "\n",
            "Epoch:    8/10    Loss: 3.17593239402771\n",
            "\n",
            "Epoch:    8/10    Loss: 3.182138163089752\n",
            "\n",
            "Epoch:    8/10    Loss: 3.174463372707367\n",
            "\n",
            "Epoch:    8/10    Loss: 3.202443871021271\n",
            "\n",
            "Epoch:    8/10    Loss: 3.221194167137146\n",
            "\n",
            "Epoch:    8/10    Loss: 3.210946252346039\n",
            "\n",
            "Epoch:    8/10    Loss: 3.198389400482178\n",
            "\n",
            "Epoch:    8/10    Loss: 3.2187889404296874\n",
            "\n",
            "Epoch:    8/10    Loss: 3.2446695761680604\n",
            "\n",
            "Epoch:    8/10    Loss: 3.249038019657135\n",
            "\n",
            "Epoch:    8/10    Loss: 3.24164244222641\n",
            "\n",
            "Epoch:    9/10    Loss: 3.1213604032011237\n",
            "\n",
            "Epoch:    9/10    Loss: 3.0465776138305665\n",
            "\n",
            "Epoch:    9/10    Loss: 3.056066141605377\n",
            "\n",
            "Epoch:    9/10    Loss: 3.0648349542617797\n",
            "\n",
            "Epoch:    9/10    Loss: 3.0715207290649413\n",
            "\n",
            "Epoch:    9/10    Loss: 3.0981252670288084\n",
            "\n",
            "Epoch:    9/10    Loss: 3.105743426322937\n",
            "\n",
            "Epoch:    9/10    Loss: 3.1278238229751585\n",
            "\n",
            "Epoch:    9/10    Loss: 3.122015781879425\n",
            "\n",
            "Epoch:    9/10    Loss: 3.0993167428970336\n",
            "\n",
            "Epoch:    9/10    Loss: 3.123038053035736\n",
            "\n",
            "Epoch:    9/10    Loss: 3.116948401927948\n",
            "\n",
            "Epoch:    9/10    Loss: 3.130546914100647\n",
            "\n",
            "Epoch:    9/10    Loss: 3.1585998315811157\n",
            "\n",
            "Epoch:    9/10    Loss: 3.1530347886085512\n",
            "\n",
            "Epoch:    9/10    Loss: 3.165642275810242\n",
            "\n",
            "Epoch:    9/10    Loss: 3.185838871479034\n",
            "\n",
            "Epoch:    9/10    Loss: 3.1827635259628297\n",
            "\n",
            "Epoch:    9/10    Loss: 3.222355616092682\n",
            "\n",
            "Epoch:    9/10    Loss: 3.199497333049774\n",
            "\n",
            "Epoch:   10/10    Loss: 3.0515568259399957\n",
            "\n",
            "Epoch:   10/10    Loss: 2.9997324647903443\n",
            "\n",
            "Epoch:   10/10    Loss: 3.0088041672706605\n",
            "\n",
            "Epoch:   10/10    Loss: 3.043478085041046\n",
            "\n",
            "Epoch:   10/10    Loss: 3.0523041830062865\n",
            "\n",
            "Epoch:   10/10    Loss: 3.0460834860801698\n",
            "\n",
            "Epoch:   10/10    Loss: 3.084716943740845\n",
            "\n",
            "Epoch:   10/10    Loss: 3.071582775592804\n",
            "\n",
            "Epoch:   10/10    Loss: 3.0651419072151183\n",
            "\n",
            "Epoch:   10/10    Loss: 3.0784781150817873\n",
            "\n",
            "Epoch:   10/10    Loss: 3.1023544268608094\n",
            "\n",
            "Epoch:   10/10    Loss: 3.071493335723877\n",
            "\n",
            "Epoch:   10/10    Loss: 3.1147971549034117\n",
            "\n",
            "Epoch:   10/10    Loss: 3.115516900062561\n",
            "\n",
            "Epoch:   10/10    Loss: 3.11203000831604\n",
            "\n",
            "Epoch:   10/10    Loss: 3.1189688606262207\n",
            "\n",
            "Epoch:   10/10    Loss: 3.1576650166511535\n",
            "\n",
            "Epoch:   10/10    Loss: 3.159255593776703\n",
            "\n",
            "Epoch:   10/10    Loss: 3.172056483745575\n",
            "\n",
            "Epoch:   10/10    Loss: 3.153827188014984\n",
            "\n",
            "Model Trained and Saved\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "def generate(rnn, prime_id, int_to_vocab, token_dict, pad_value, predict_len=100):\n",
        "    \"\"\"\n",
        "    Generate text using the neural network\n",
        "    :param decoder: The PyTorch Module that holds the trained neural network\n",
        "    :param prime_id: The word id to start the first prediction\n",
        "    :param int_to_vocab: Dict of word id keys to word values\n",
        "    :param token_dict: Dict of puncuation tokens keys to puncuation values\n",
        "    :param pad_value: The value used to pad a sequence\n",
        "    :param predict_len: The length of text to generate\n",
        "    :return: The generated text\n",
        "    \"\"\"\n",
        "    rnn.eval()\n",
        "\n",
        "    # create a sequence (batch_size=1) with the prime_id\n",
        "    current_seq = np.full((1, sequence_length), pad_value)\n",
        "    current_seq[-1][-1] = prime_id\n",
        "    predicted = [int_to_vocab[prime_id]]\n",
        "\n",
        "    for _ in range(predict_len):\n",
        "        if train_on_gpu:\n",
        "            current_seq = torch.LongTensor(current_seq).cuda()\n",
        "        else:\n",
        "            current_seq = torch.LongTensor(current_seq)\n",
        "\n",
        "        # initialize the hidden state\n",
        "        hidden = rnn.init_hidden(current_seq.size(0))\n",
        "\n",
        "        # get the output of the rnn\n",
        "        output, _ = rnn(current_seq, hidden)\n",
        "\n",
        "        # get the next word probabilities\n",
        "        p = F.softmax(output, dim=1).data\n",
        "        if(train_on_gpu):\n",
        "            p = p.cpu() # move to cpu\n",
        "\n",
        "        # use top_k sampling to get the index of the next word\n",
        "        top_k = 5\n",
        "        p, top_i = p.topk(top_k)\n",
        "        top_i = top_i.numpy().squeeze()\n",
        "\n",
        "        # select the likely next word index with some element of randomness\n",
        "        p = p.numpy().squeeze()\n",
        "        word_i = np.random.choice(top_i, p=p/p.sum())\n",
        "\n",
        "        # retrieve that word from the dictionary\n",
        "        word = int_to_vocab[word_i]\n",
        "        predicted.append(word)\n",
        "\n",
        "        # the generated word becomes the next \"current sequence\" and the cycle can continue\n",
        "        current_seq = np.roll(current_seq.cpu(), -1, 1)\n",
        "        current_seq[-1][-1] = word_i\n",
        "\n",
        "    gen_sentences = ' '.join(predicted)\n",
        "\n",
        "    # Replace punctuation tokens\n",
        "    for key, token in token_dict.items():\n",
        "        ending = ' ' if key in ['\\n', '(', '\"'] else ''\n",
        "        gen_sentences = gen_sentences.replace(' ' + token.lower(), key)\n",
        "    gen_sentences = gen_sentences.replace('\\n ', '\\n')\n",
        "    gen_sentences = gen_sentences.replace('( ', '(')\n",
        "\n",
        "    # return all the sentences\n",
        "    return gen_sentences"
      ],
      "metadata": {
        "id": "qT7EEUAHzd0Z"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gen_length = 50 # modify the length to your preference\n",
        "prime_words = ['tesla'] # name for starting the script\n",
        "\n",
        "for prime_word in prime_words:\n",
        "    pad_word = SPECIAL_WORDS['PADDING']\n",
        "    generated_script = generate(trained_rnn, vocab_to_int[prime_word], int_to_vocab, token_dict, vocab_to_int[pad_word], gen_length)\n",
        "    print(generated_script)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9OTL2u-Y2fPu",
        "outputId": "095d87e7-c369-4497-c014-78afb01d9da0"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tesla and contrarian ideas\n",
            "the chemist's 'high? high? low' closed? end fund report: april 2019\n",
            "alpine global dynamic dividend fund goes ex? dividend tomorrow\n",
            "tracking lou simpson's sq advisors portfolio? q4 2018 update\n",
            "a. o. smith declares $0. 22\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZECeAe5tAt2o"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}